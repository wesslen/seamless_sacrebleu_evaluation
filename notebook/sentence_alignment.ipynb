{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5hRxLd92uKCsyrNzr/fFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebook/sentence_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english = \"\"\"\n",
        "Machine learning is transforming the way we interact with technology. It powers everything from recommendation systems to autonomous vehicles.\n",
        "\n",
        "## Basic Concepts\n",
        "\n",
        "Neural networks are inspired by the human brain! They consist of interconnected nodes that process information in layers.\n",
        "\n",
        "### Types of Learning\n",
        "\n",
        "Supervised learning requires labeled data.   Multiple spaces    here should be cleaned.\n",
        "\n",
        "Unsupervised learning finds patterns without labels.\n",
        "\n",
        "* This is a bullet point.\n",
        "* x\n",
        "\n",
        "Semi-supervised learning combines both approaches...\n",
        "\n",
        "## Advanced Topics\n",
        "\n",
        "Deep learning has revolutionized computer vision and natural language processing.\n",
        "\n",
        "Transfer learning allows models to apply knowledge from one domain to another.\n",
        "\n",
        "Contact: info@example.com\n",
        "\"\"\"\n",
        "\n",
        "spanish = \"\"\"\n",
        "El aprendizaje automático está transformando la forma en que interactuamos con la tecnología. Impulsa todo, desde sistemas de recomendación hasta vehículos autónomos.\n",
        "\n",
        "## Conceptos Básicos\n",
        "\n",
        "¡Las redes neuronales están inspiradas en el cerebro humano! Consisten en nodos interconectados que procesan información en capas.\n",
        "\n",
        "### Tipos de Aprendizaje\n",
        "\n",
        "El aprendizaje supervisado requiere datos etiquetados.    Múltiples espacios    aquí deben limpiarse.\n",
        "\n",
        "El aprendizaje no supervisado encuentra patrones sin etiquetas.\n",
        "\n",
        "* Este es un punto de viñeta.\n",
        "* x\n",
        "\n",
        "El aprendizaje semisupervisado combina ambos enfoques...\n",
        "\n",
        "## Temas Avanzados\n",
        "\n",
        "El aprendizaje profundo ha revolucionado la visión por computadora y el procesamiento del lenguaje natural.\n",
        "\n",
        "La transferencia de aprendizaje permite que los modelos apliquen el conocimiento de un dominio a otro.\n",
        "\n",
        "Contacto: info@example.com\n",
        "\"\"\"\n",
        "\n",
        "# save english as english.txt\n",
        "with open('english.txt', 'w') as f:\n",
        "    f.write(english)\n",
        "\n",
        "# save spanish as spanish.txt\n",
        "with open('spanish.txt', 'w') as f:\n",
        "    f.write(spanish)"
      ],
      "metadata": {
        "id": "ceqH1O4TpVT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import math\n",
        "import logging\n",
        "import chardet\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('sentence_aligner.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FileValidationError(Exception):\n",
        "    \"\"\"Custom exception for file validation errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class SentencePair:\n",
        "    source: str\n",
        "    target: str\n",
        "    source_index: List[int]  # Changed to list for n:m alignments\n",
        "    target_index: List[int]\n",
        "    alignment_score: float = 0.0\n",
        "\n",
        "class GaleChurchAligner:\n",
        "    \"\"\"Implementation of Gale-Church alignment algorithm.\"\"\"\n",
        "\n",
        "    # Constants for Gale-Church algorithm\n",
        "    MEAN_CHARACTERS_RATIO = 1\n",
        "    VARIANCE_CHARACTERS_RATIO = 6.8\n",
        "\n",
        "    def __init__(self):\n",
        "        self.log_prob_tables = {}\n",
        "\n",
        "    def char_length_ratio(self, source_len: int, target_len: int) -> float:\n",
        "        \"\"\"Calculate the log probability of character length ratio.\"\"\"\n",
        "        try:\n",
        "            ratio = (target_len - source_len * self.MEAN_CHARACTERS_RATIO) / \\\n",
        "                    math.sqrt(source_len * self.VARIANCE_CHARACTERS_RATIO)\n",
        "            return -math.log(1 + ratio * ratio)\n",
        "        except (ValueError, ZeroDivisionError):\n",
        "            return float('-inf')\n",
        "\n",
        "    def calculate_alignment_cost(\n",
        "        self,\n",
        "        source_block: List[str],\n",
        "        target_block: List[str]\n",
        "    ) -> float:\n",
        "        \"\"\"Calculate alignment cost for blocks of sentences.\"\"\"\n",
        "        source_len = sum(len(s) for s in source_block)\n",
        "        target_len = sum(len(t) for t in target_block)\n",
        "\n",
        "        if not source_len or not target_len:\n",
        "            return float('inf')\n",
        "\n",
        "        return -self.char_length_ratio(source_len, target_len)\n",
        "\n",
        "    def align_blocks(\n",
        "        self,\n",
        "        source_sents: List[str],\n",
        "        target_sents: List[str]\n",
        "    ) -> List[Tuple[List[int], List[int], float]]:\n",
        "        \"\"\"\n",
        "        Implement dynamic programming to find optimal alignment.\n",
        "        Returns list of (source_indices, target_indices, score).\n",
        "        \"\"\"\n",
        "        n, m = len(source_sents), len(target_sents)\n",
        "\n",
        "        # Initialize DP tables\n",
        "        dp = defaultdict(lambda: float('inf'))\n",
        "        dp[0, 0] = 0\n",
        "        back = {}\n",
        "\n",
        "        # Possible alignment patterns (1-1, 1-2, 2-1, 2-2)\n",
        "        patterns = [(1,1), (1,2), (2,1), (2,2)]\n",
        "\n",
        "        # Fill DP table\n",
        "        for i in range(n + 1):\n",
        "            for j in range(m + 1):\n",
        "                if i == 0 and j == 0:\n",
        "                    continue\n",
        "\n",
        "                for si, ti in patterns:\n",
        "                    if i >= si and j >= ti:\n",
        "                        source_block = source_sents[i-si:i]\n",
        "                        target_block = target_sents[j-ti:j]\n",
        "                        cost = self.calculate_alignment_cost(source_block, target_block)\n",
        "\n",
        "                        if dp[i-si, j-ti] + cost < dp[i, j]:\n",
        "                            dp[i, j] = dp[i-si, j-ti] + cost\n",
        "                            back[i, j] = (si, ti)\n",
        "\n",
        "        # Reconstruct alignment\n",
        "        alignments = []\n",
        "        i, j = n, m\n",
        "        while i > 0 or j > 0:\n",
        "            si, ti = back.get((i, j), (1, 1))\n",
        "            source_indices = list(range(i-si, i))\n",
        "            target_indices = list(range(j-ti, j))\n",
        "            score = dp[i, j] - dp[i-si, j-ti]\n",
        "            alignments.append((source_indices, target_indices, score))\n",
        "            i, j = i-si, j-ti\n",
        "\n",
        "        return list(reversed(alignments))\n",
        "\n",
        "class SentenceAligner:\n",
        "    def __init__(self):\n",
        "        self.sentence_endings = r'[.!?।。؟।෴।۔]+'\n",
        "        self.sentence_pattern = re.compile(\n",
        "            fr'([^{self.sentence_endings}]+?{self.sentence_endings}|[^{self.sentence_endings}]+$)'\n",
        "        )\n",
        "        self.gale_church = GaleChurchAligner()\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_encoding(file_path: Path) -> str:\n",
        "        \"\"\"Detect file encoding.\"\"\"\n",
        "        with open(file_path, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        if result['confidence'] < 0.7:\n",
        "            logger.warning(f\"Low confidence in encoding detection for {file_path}\")\n",
        "        return result['encoding']\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_file_contents(text: str) -> bool:\n",
        "        \"\"\"Validate file contents.\"\"\"\n",
        "        if not text.strip():\n",
        "            raise FileValidationError(\"File is empty or contains only whitespace\")\n",
        "        if len(text) > 10_000_000:  # 10MB limit\n",
        "            raise FileValidationError(\"File exceeds size limit\")\n",
        "        return True\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Apply whitespace cleaning rules to text.\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def is_valid_sentence(self, sentence: str) -> bool:\n",
        "        \"\"\"Check if sentence meets inclusion criteria.\"\"\"\n",
        "        sentence = self.clean_text(sentence)\n",
        "\n",
        "        # Check word count\n",
        "        words = sentence.split()\n",
        "        if len(words) <= 1 or len(words) > 50:\n",
        "            return False\n",
        "\n",
        "        # Check alphanumeric ratio\n",
        "        alphanumeric_chars = sum(c.isalnum() for c in sentence)\n",
        "        try:\n",
        "            if alphanumeric_chars / len(sentence) < 0.01:\n",
        "                return False\n",
        "        except ZeroDivisionError:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def tokenize_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        sentences = self.sentence_pattern.findall(text)\n",
        "        return [self.clean_text(sent) for sent in sentences if sent.strip()]\n",
        "\n",
        "    def align_sentences(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        target_text: str\n",
        "    ) -> Tuple[List[Dict], List[Dict]]:\n",
        "        \"\"\"\n",
        "        Align sentences using Gale-Church algorithm.\n",
        "        Returns tuple of (aligned_pairs, excluded_pairs).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Tokenize both texts\n",
        "            source_sentences = self.tokenize_sentences(source_text)\n",
        "            target_sentences = self.tokenize_sentences(target_text)\n",
        "\n",
        "            logger.info(f\"Tokenized {len(source_sentences)} source and {len(target_sentences)} target sentences\")\n",
        "\n",
        "            # Get alignments using Gale-Church\n",
        "            alignments = self.gale_church.align_blocks(source_sentences, target_sentences)\n",
        "\n",
        "            aligned_pairs = []\n",
        "            excluded_pairs = []\n",
        "\n",
        "            for source_indices, target_indices, score in alignments:\n",
        "                source_block = [source_sentences[i] for i in source_indices]\n",
        "                target_block = [target_sentences[i] for i in target_indices]\n",
        "\n",
        "                pair_dict = {\n",
        "                    \"source\": \" \".join(source_block),\n",
        "                    \"target\": \" \".join(target_block),\n",
        "                    \"source_index\": source_indices,\n",
        "                    \"target_index\": target_indices,\n",
        "                    \"alignment_score\": score\n",
        "                }\n",
        "\n",
        "                # Check if any sentence in either block should be excluded\n",
        "                should_exclude = any(\n",
        "                    not self.is_valid_sentence(sent)\n",
        "                    for sent in source_block + target_block\n",
        "                )\n",
        "\n",
        "                if should_exclude:\n",
        "                    excluded_pairs.append(pair_dict)\n",
        "                else:\n",
        "                    aligned_pairs.append(pair_dict)\n",
        "\n",
        "            logger.info(f\"Aligned {len(aligned_pairs)} pairs, excluded {len(excluded_pairs)} pairs\")\n",
        "            return aligned_pairs, excluded_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in sentence alignment: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def process_files(\n",
        "    source_path: str,\n",
        "    target_path: str,\n",
        "    output_dir: str\n",
        ") -> Tuple[Path, Path]:\n",
        "    \"\"\"\n",
        "    Process source and target files with enhanced error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert to Path objects\n",
        "        source_path = Path(source_path)\n",
        "        target_path = Path(target_path)\n",
        "        output_dir = Path(output_dir)\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Detect file encodings\n",
        "        source_encoding = SentenceAligner.detect_encoding(source_path)\n",
        "        target_encoding = SentenceAligner.detect_encoding(target_path)\n",
        "\n",
        "        logger.info(f\"Detected encodings: source={source_encoding}, target={target_encoding}\")\n",
        "\n",
        "        # Read input files\n",
        "        with open(source_path, 'r', encoding=source_encoding) as f:\n",
        "            source_text = f.read()\n",
        "        with open(target_path, 'r', encoding=target_encoding) as f:\n",
        "            target_text = f.read()\n",
        "\n",
        "        # Validate file contents\n",
        "        SentenceAligner.validate_file_contents(source_text)\n",
        "        SentenceAligner.validate_file_contents(target_text)\n",
        "\n",
        "        # Process texts\n",
        "        aligner = SentenceAligner()\n",
        "        aligned_pairs, excluded_pairs = aligner.align_sentences(source_text, target_text)\n",
        "\n",
        "        logger.info(f\"Processed {len(aligned_pairs)} aligned pairs and {len(excluded_pairs)} excluded pairs\")\n",
        "\n",
        "        # Define output paths\n",
        "        aligned_path = output_dir / 'aligned.jsonl'\n",
        "        excluded_path = output_dir / 'excluded.jsonl'\n",
        "\n",
        "        # Write output files\n",
        "        with open(aligned_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in aligned_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        with open(excluded_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in excluded_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        logger.info(f\"Successfully wrote output files to {output_dir}\")\n",
        "        return aligned_path, excluded_path\n",
        "\n",
        "    except FileValidationError as e:\n",
        "        logger.error(f\"File validation error: {str(e)}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error in file processing: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Example usage in Jupyter notebook\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        source_file = \"spanish.txt\"\n",
        "        target_file = \"english.txt\"\n",
        "        output_directory = \"output\"\n",
        "\n",
        "        aligned_file, excluded_file = process_files(source_file, target_file, output_directory)\n",
        "        print(f\"Aligned sentences written to: {aligned_file}\")\n",
        "        print(f\"Excluded sentences written to: {excluded_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to process files: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlUmsaiJlgqk",
        "outputId": "e950a9df-c4c0-4111-c4a1-dccbb28328eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned sentences written to: output/aligned.jsonl\n",
            "Excluded sentences written to: output/excluded.jsonl\n"
          ]
        }
      ]
    }
  ]
}