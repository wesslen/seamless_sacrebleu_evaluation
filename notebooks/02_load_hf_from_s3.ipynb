{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsevcAKjwH/paJRYchUR6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/02_load_hf_from_s3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore.config import Config\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from contextlib import contextmanager\n",
        "from io import BytesIO\n",
        "from tempfile import TemporaryDirectory\n",
        "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
        "from typing import Tuple, Optional, Union, List, Dict\n",
        "from pathlib import Path\n",
        "import urllib3\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "class NotebookLogger:\n",
        "    \"\"\"Custom logger for Jupyter notebooks with colored output\"\"\"\n",
        "\n",
        "    COLORS = {\n",
        "        'INFO': '#0066cc',\n",
        "        'DEBUG': '#666666',\n",
        "        'WARNING': '#ff9900',\n",
        "        'ERROR': '#cc0000',\n",
        "        'SUCCESS': '#009933'\n",
        "    }\n",
        "\n",
        "    def __init__(self, enable_debug=False):\n",
        "        self.enable_debug = enable_debug\n",
        "\n",
        "    def _log(self, level: str, message: str):\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        color = self.COLORS.get(level, '#000000')\n",
        "        display(HTML(\n",
        "            f'<pre style=\"margin:0; padding:2px 0; color: {color}\">'\n",
        "            f'[{timestamp}] {level}: {message}'\n",
        "            '</pre>'\n",
        "        ))\n",
        "\n",
        "    def info(self, message: str):\n",
        "        self._log('INFO', message)\n",
        "\n",
        "    def debug_log(self, message: str):\n",
        "        if self.enable_debug:\n",
        "            self._log('DEBUG', message)\n",
        "\n",
        "    def warning(self, message: str):\n",
        "        self._log('WARNING', message)\n",
        "\n",
        "    def error(self, message: str):\n",
        "        self._log('ERROR', message)\n",
        "\n",
        "    def success(self, message: str):\n",
        "        self._log('SUCCESS', message)\n",
        "\n",
        "def get_s3_client(endpoint_url: Optional[str] = None, verify_ssl: bool = True):\n",
        "    \"\"\"Create an S3 client with configurable SSL verification.\"\"\"\n",
        "    if not verify_ssl:\n",
        "        warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    config = Config(retries=dict(max_attempts=3))\n",
        "\n",
        "    return boto3.client(\n",
        "        's3',\n",
        "        endpoint_url=endpoint_url,\n",
        "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
        "        verify=verify_ssl,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "def find_model_files(files: List[str], logger: NotebookLogger) -> Tuple[List[str], bool]:\n",
        "    \"\"\"\n",
        "    Find model files following either pattern:\n",
        "    - model.safetensors\n",
        "    - model-00001-of-00002.safetensors (sharded)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (list of files, is_sharded)\n",
        "    \"\"\"\n",
        "    # First look for single safetensors file\n",
        "    single_file = [f for f in files if f.endswith('model.safetensors')]\n",
        "    if single_file:\n",
        "        logger.debug_log(\"Found single safetensors file\")\n",
        "        return single_file, False\n",
        "\n",
        "    # Look for sharded files\n",
        "    sharded_pattern = re.compile(r'model-\\d{5}-of-\\d{5}\\.safetensors$')\n",
        "    sharded_files = [f for f in files if sharded_pattern.search(os.path.basename(f))]\n",
        "\n",
        "    if sharded_files:\n",
        "        logger.debug_log(f\"Found {len(sharded_files)} sharded safetensors files\")\n",
        "        # Sort to ensure consistent ordering\n",
        "        return sorted(sharded_files), True\n",
        "\n",
        "    logger.debug_log(\"No safetensors files found\")\n",
        "    return [], False\n",
        "\n",
        "def load_model_from_s3(\n",
        "    bucket: str,\n",
        "    path_to_model: str,\n",
        "    endpoint_url: Optional[str] = None,\n",
        "    verify_ssl: bool = True,\n",
        "    force_bin: bool = False,\n",
        "    enable_debug: bool = False\n",
        ") -> Tuple[Union[AutoModel, None], Union[AutoTokenizer, None]]:\n",
        "    \"\"\"\n",
        "    Load a model and tokenizer from S3 storage, supporting both single and sharded safetensors.\n",
        "    \"\"\"\n",
        "    logger = NotebookLogger(enable_debug=enable_debug)\n",
        "    logger.info(f\"Starting model load from bucket: {bucket}, path: {path_to_model}\")\n",
        "\n",
        "    s3_client = get_s3_client(endpoint_url, verify_ssl)\n",
        "\n",
        "    # List all files in the model directory\n",
        "    logger.info(\"Listing files in bucket...\")\n",
        "    files = []\n",
        "    paginator = s3_client.get_paginator('list_objects_v2')\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=path_to_model):\n",
        "        if 'Contents' in page:\n",
        "            files.extend(obj['Key'] for obj in page['Contents'])\n",
        "\n",
        "    logger.debug_log(f\"Found {len(files)} total files\")\n",
        "    for file in files:\n",
        "        logger.debug_log(f\"Found file: {file}\")\n",
        "\n",
        "    # Find model files\n",
        "    safetensors_files, is_sharded = find_model_files(files, logger)\n",
        "    bin_files = [f for f in files if f.endswith('.bin')]\n",
        "\n",
        "    logger.info(f\"Found {len(safetensors_files)} safetensor files (sharded: {is_sharded}) and {len(bin_files)} bin files\")\n",
        "\n",
        "    with TemporaryDirectory() as temp_dir:\n",
        "        temp_path = Path(temp_dir)\n",
        "        logger.debug_log(f\"Created temporary directory: {temp_dir}\")\n",
        "\n",
        "        # Download config.json first\n",
        "        config_file = next((f for f in files if f.endswith('config.json')), None)\n",
        "        if config_file:\n",
        "            config_path = temp_path / 'config.json'\n",
        "            logger.info(f\"Downloading config file: {config_file}\")\n",
        "            with open(config_path, 'wb') as out:\n",
        "                obj = s3_client.get_object(Bucket=bucket, Key=config_file)\n",
        "                out.write(obj['Body'].read())\n",
        "            logger.success(\"Config file downloaded successfully\")\n",
        "\n",
        "            # Read config to check tokenizer type\n",
        "            with open(config_path) as f:\n",
        "                config_data = json.load(f)\n",
        "                tokenizer_type = config_data.get('tokenizer_class', '')\n",
        "                logger.debug_log(f\"Detected tokenizer type: {tokenizer_type}\")\n",
        "        else:\n",
        "            logger.error(\"No config.json found!\")\n",
        "            raise Exception(\"No config.json found in model directory\")\n",
        "\n",
        "        # Handle model weights\n",
        "        if safetensors_files and not force_bin:\n",
        "            logger.info(\"Using safetensors format for model loading\")\n",
        "            for file in safetensors_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading safetensors file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "\n",
        "            if is_sharded:\n",
        "                # Create an index file for sharded safetensors\n",
        "                index_data = {\n",
        "                    \"metadata\": {\"total_size\": 0},\n",
        "                    \"weight_map\": {\n",
        "                        \"\": \"model-00001-of-00002.safetensors\"\n",
        "                    }\n",
        "                }\n",
        "                with open(temp_path / \"model.safetensors.index.json\", \"w\") as f:\n",
        "                    json.dump(index_data, f)\n",
        "                logger.debug_log(\"Created index file for sharded safetensors\")\n",
        "\n",
        "        elif bin_files:\n",
        "            logger.info(\"Using .bin format for model loading\")\n",
        "            for file in bin_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading bin file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "        else:\n",
        "            logger.error(\"No model weights files found!\")\n",
        "            raise Exception(\"No model weights files (safetensors or bin) found\")\n",
        "\n",
        "        # Download tokenizer files with special handling for SentencePiece\n",
        "        tokenizer_extensions = [\n",
        "            'tokenizer.json',\n",
        "            'tokenizer_config.json',\n",
        "            'special_tokens_map.json',\n",
        "            'vocab.json',\n",
        "            'merges.txt',\n",
        "            'tokenizer.model'  # SentencePiece model file\n",
        "        ]\n",
        "\n",
        "        tokenizer_files = [f for f in files if any(f.endswith(ext) for ext in tokenizer_extensions)]\n",
        "\n",
        "        has_tokenizer_files = False\n",
        "        if tokenizer_files:\n",
        "            logger.info(f\"Found {len(tokenizer_files)} tokenizer files\")\n",
        "            for file in tokenizer_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading tokenizer file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "                has_tokenizer_files = True\n",
        "        else:\n",
        "            logger.warning(\"No tokenizer files found\")\n",
        "\n",
        "        # Debug: list all files in temp directory\n",
        "        logger.debug_log(\"Files in temporary directory:\")\n",
        "        for file in Path(temp_dir).rglob('*'):\n",
        "            if file.is_file():\n",
        "                logger.debug_log(f\"  {file.relative_to(temp_dir)}\")\n",
        "\n",
        "        # Load the model and tokenizer\n",
        "        try:\n",
        "            logger.info(\"Loading model from temporary directory\")\n",
        "            model = AutoModel.from_pretrained(\n",
        "                str(temp_path),  # Convert path to string\n",
        "                local_files_only=True,\n",
        "                use_safetensors=not force_bin,\n",
        "            )\n",
        "            logger.success(\"Model loaded successfully\")\n",
        "\n",
        "            tokenizer = None\n",
        "            if has_tokenizer_files:\n",
        "                try:\n",
        "                    logger.info(\"Loading tokenizer\")\n",
        "                    # Special handling for SentencePiece models\n",
        "                    if 'LlamaTokenizer' in tokenizer_type or any(f.endswith('tokenizer.model') for f in tokenizer_files):\n",
        "                        logger.debug_log(\"Using LlamaTokenizer settings\")\n",
        "                        from transformers import LlamaTokenizer\n",
        "                        tokenizer = LlamaTokenizer.from_pretrained(\n",
        "                            str(temp_path),\n",
        "                            local_files_only=True\n",
        "                        )\n",
        "                    else:\n",
        "                        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            str(temp_path),\n",
        "                            local_files_only=True\n",
        "                        )\n",
        "                    logger.success(\"Tokenizer loaded successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to load tokenizer: {str(e)}\")\n",
        "\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "BS8xd74ST0VN",
        "outputId": "33f435ec-3af5-416d-ed82-c6d035806363"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'boto3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-480a0e623ff2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with debugging enabled\n",
        "try:\n",
        "    model, tokenizer = load_model_from_s3(\n",
        "        bucket=\"my-bucket\",\n",
        "        path_to_model=\"models/my-model\",\n",
        "        endpoint_url=\"https://my-storage-endpoint\",\n",
        "        verify_ssl=False,\n",
        "        enable_debug=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model: {e}\")"
      ],
      "metadata": {
        "id": "_ChlOgdWJ8ET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}