{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh+4glh08L7grLttkQcH8X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/02_load_hf_from_s3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore.config import Config\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from contextlib import contextmanager\n",
        "from io import BytesIO\n",
        "from tempfile import TemporaryDirectory\n",
        "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
        "from typing import Tuple, Optional, Union, List, Dict\n",
        "from pathlib import Path\n",
        "import urllib3\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "class NotebookLogger:\n",
        "    \"\"\"Custom logger for Jupyter notebooks with colored output\"\"\"\n",
        "\n",
        "    COLORS = {\n",
        "        'INFO': '#0066cc',\n",
        "        'DEBUG': '#666666',\n",
        "        'WARNING': '#ff9900',\n",
        "        'ERROR': '#cc0000',\n",
        "        'SUCCESS': '#009933'\n",
        "    }\n",
        "\n",
        "    def __init__(self, enable_debug=False):\n",
        "        self.enable_debug = enable_debug\n",
        "\n",
        "    def _log(self, level: str, message: str):\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        color = self.COLORS.get(level, '#000000')\n",
        "        display(HTML(\n",
        "            f'<pre style=\"margin:0; padding:2px 0; color: {color}\">'\n",
        "            f'[{timestamp}] {level}: {message}'\n",
        "            '</pre>'\n",
        "        ))\n",
        "\n",
        "    def info(self, message: str):\n",
        "        self._log('INFO', message)\n",
        "\n",
        "    def debug_log(self, message: str):\n",
        "        if self.enable_debug:\n",
        "            self._log('DEBUG', message)\n",
        "\n",
        "    def warning(self, message: str):\n",
        "        self._log('WARNING', message)\n",
        "\n",
        "    def error(self, message: str):\n",
        "        self._log('ERROR', message)\n",
        "\n",
        "    def success(self, message: str):\n",
        "        self._log('SUCCESS', message)\n",
        "\n",
        "def get_s3_client(endpoint_url: Optional[str] = None, verify_ssl: bool = True):\n",
        "    \"\"\"Create an S3 client with configurable SSL verification.\"\"\"\n",
        "    if not verify_ssl:\n",
        "        warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    config = Config(retries=dict(max_attempts=3))\n",
        "\n",
        "    return boto3.client(\n",
        "        's3',\n",
        "        endpoint_url=endpoint_url,\n",
        "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
        "        verify=verify_ssl,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "def find_model_files(files: List[str], logger: NotebookLogger) -> Tuple[List[str], bool]:\n",
        "    \"\"\"\n",
        "    Find model files following either pattern:\n",
        "    - model.safetensors\n",
        "    - model-00001-of-00002.safetensors (sharded)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (list of files, is_sharded)\n",
        "    \"\"\"\n",
        "    # First look for single safetensors file\n",
        "    single_file = [f for f in files if f.endswith('model.safetensors')]\n",
        "    if single_file:\n",
        "        logger.debug_log(\"Found single safetensors file\")\n",
        "        return single_file, False\n",
        "\n",
        "    # Look for sharded files\n",
        "    sharded_pattern = re.compile(r'model-\\d{5}-of-\\d{5}\\.safetensors$')\n",
        "    sharded_files = [f for f in files if sharded_pattern.search(os.path.basename(f))]\n",
        "\n",
        "    if sharded_files:\n",
        "        logger.debug_log(f\"Found {len(sharded_files)} sharded safetensors files\")\n",
        "        # Sort to ensure consistent ordering\n",
        "        return sorted(sharded_files), True\n",
        "\n",
        "    logger.debug_log(\"No safetensors files found\")\n",
        "    return [], False\n",
        "\n",
        "def load_model_from_s3(\n",
        "    bucket: str,\n",
        "    path_to_model: str,\n",
        "    endpoint_url: Optional[str] = None,\n",
        "    verify_ssl: bool = True,\n",
        "    force_bin: bool = False,\n",
        "    enable_debug: bool = False,\n",
        "    model_class: Optional[type] = None\n",
        ") -> Tuple[Union[AutoModel, None], Union[AutoTokenizer, None]]:\n",
        "    \"\"\"\n",
        "    Load a model and tokenizer from S3 storage, supporting both single and sharded safetensors.\n",
        "\n",
        "    Args:\n",
        "        bucket (str): S3 bucket name\n",
        "        path_to_model (str): Path to model directory in bucket\n",
        "        endpoint_url (str, optional): Custom S3 endpoint URL\n",
        "        verify_ssl (bool): Whether to verify SSL certificates\n",
        "        force_bin (bool): Force using .bin format even if .safetensors is available\n",
        "        enable_debug (bool): Enable detailed debug logging\n",
        "        model_class (type, optional): Specific model class to use (e.g., AutoModelForCausalLM)\n",
        "    \"\"\"\n",
        "    logger = NotebookLogger(enable_debug=enable_debug)\n",
        "    logger.info(f\"Starting model load from bucket: {bucket}, path: {path_to_model}\")\n",
        "\n",
        "    s3_client = get_s3_client(endpoint_url, verify_ssl)\n",
        "\n",
        "    # List all files in the model directory\n",
        "    logger.info(\"Listing files in bucket...\")\n",
        "    files = []\n",
        "    paginator = s3_client.get_paginator('list_objects_v2')\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=path_to_model):\n",
        "        if 'Contents' in page:\n",
        "            files.extend(obj['Key'] for obj in page['Contents'])\n",
        "\n",
        "    logger.debug_log(f\"Found {len(files)} total files\")\n",
        "    for file in files:\n",
        "        logger.debug_log(f\"Found file: {file}\")\n",
        "\n",
        "    # Find model files\n",
        "    safetensors_files, is_sharded = find_model_files(files, logger)\n",
        "    bin_files = [f for f in files if f.endswith('.bin')]\n",
        "\n",
        "    logger.info(f\"Found {len(safetensors_files)} safetensor files (sharded: {is_sharded}) and {len(bin_files)} bin files\")\n",
        "\n",
        "    with TemporaryDirectory() as temp_dir:\n",
        "        temp_path = Path(temp_dir)\n",
        "        logger.debug_log(f\"Created temporary directory: {temp_dir}\")\n",
        "\n",
        "        # Download config.json first\n",
        "        config_file = next((f for f in files if f.endswith('config.json')), None)\n",
        "        if config_file:\n",
        "            config_path = temp_path / 'config.json'\n",
        "            logger.info(f\"Downloading config file: {config_file}\")\n",
        "            with open(config_path, 'wb') as out:\n",
        "                obj = s3_client.get_object(Bucket=bucket, Key=config_file)\n",
        "                out.write(obj['Body'].read())\n",
        "            logger.success(\"Config file downloaded successfully\")\n",
        "\n",
        "            # Read config to check model and tokenizer type\n",
        "            with open(config_path) as f:\n",
        "                config_data = json.load(f)\n",
        "                model_type = config_data.get('model_type', '').lower()\n",
        "                logger.debug_log(f\"Detected model type: {model_type}\")\n",
        "        else:\n",
        "            logger.error(\"No config.json found!\")\n",
        "            raise Exception(\"No config.json found in model directory\")\n",
        "\n",
        "        # Handle model weights\n",
        "        if safetensors_files and not force_bin:\n",
        "            logger.info(\"Using safetensors format for model loading\")\n",
        "            for file in safetensors_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading safetensors file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "\n",
        "            if is_sharded:\n",
        "                index_data = {\n",
        "                    \"metadata\": {\"total_size\": 0},\n",
        "                    \"weight_map\": {\n",
        "                        \"\": \"model-00001-of-00002.safetensors\"\n",
        "                    }\n",
        "                }\n",
        "                with open(temp_path / \"model.safetensors.index.json\", \"w\") as f:\n",
        "                    json.dump(index_data, f)\n",
        "                logger.debug_log(\"Created index file for sharded safetensors\")\n",
        "\n",
        "        elif bin_files:\n",
        "            logger.info(\"Using .bin format for model loading\")\n",
        "            for file in bin_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading bin file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "        else:\n",
        "            logger.error(\"No model weights files found!\")\n",
        "            raise Exception(\"No model weights files (safetensors or bin) found\")\n",
        "\n",
        "        # Download tokenizer files\n",
        "        tokenizer_files = [f for f in files if any(f.endswith(ext) for ext in [\n",
        "            'tokenizer.json',\n",
        "            'tokenizer_config.json',\n",
        "            'special_tokens_map.json',\n",
        "            'vocab.json',\n",
        "            'merges.txt',\n",
        "            'tokenizer.model'\n",
        "        ])]\n",
        "\n",
        "        has_tokenizer_files = False\n",
        "        if tokenizer_files:\n",
        "            logger.info(f\"Found {len(tokenizer_files)} tokenizer files\")\n",
        "            for file in tokenizer_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                logger.info(f\"Downloading tokenizer file: {file}\")\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "                logger.success(f\"Downloaded: {relative_path}\")\n",
        "                has_tokenizer_files = True\n",
        "        else:\n",
        "            logger.warning(\"No tokenizer files found\")\n",
        "\n",
        "        # Debug: list all files in temp directory\n",
        "        logger.debug_log(\"Files in temporary directory:\")\n",
        "        for file in Path(temp_dir).rglob('*'):\n",
        "            if file.is_file():\n",
        "                logger.debug_log(f\"  {file.relative_to(temp_dir)}\")\n",
        "\n",
        "        # Load the model and tokenizer\n",
        "        try:\n",
        "            logger.info(\"Loading model from temporary directory\")\n",
        "\n",
        "            if model_class:\n",
        "                ModelClass = model_class\n",
        "            elif model_type == 'llama':\n",
        "                from transformers import LlamaForCausalLM\n",
        "                ModelClass = LlamaForCausalLM\n",
        "            else:\n",
        "                from transformers import AutoModel\n",
        "                ModelClass = AutoModel\n",
        "\n",
        "            model = ModelClass.from_pretrained(\n",
        "                str(temp_path),\n",
        "                local_files_only=True,\n",
        "                use_safetensors=not force_bin,\n",
        "            )\n",
        "            logger.success(\"Model loaded successfully\")\n",
        "\n",
        "            tokenizer = None\n",
        "            if has_tokenizer_files:\n",
        "                try:\n",
        "                    logger.info(\"Loading tokenizer\")\n",
        "\n",
        "                    # For Llama models, always use LlamaTokenizerFast\n",
        "                    if model_type == 'llama':\n",
        "                        from transformers import LlamaTokenizerFast\n",
        "                        tokenizer = LlamaTokenizerFast.from_pretrained(\n",
        "                            str(temp_path),\n",
        "                            local_files_only=True\n",
        "                        )\n",
        "                    else:\n",
        "                        from transformers import AutoTokenizer\n",
        "                        tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            str(temp_path),\n",
        "                            local_files_only=True\n",
        "                        )\n",
        "                    logger.success(\"Tokenizer loaded successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Failed to load tokenizer: {str(e)}\")\n",
        "\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "BS8xd74ST0VN",
        "outputId": "bd7bf12a-cc0e-44c3-a58a-9f8de1f0295c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'boto3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ec4d024a0521>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # ... [previous code remains the same until tokenizer loading] ...\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    try:\n",
        "        logger.info(\"Loading model from temporary directory\")\n",
        "        if model_class:\n",
        "            logger.debug_log(f\"Using specific model class: {model_class}\")\n",
        "            if model_class == \"LlamaForCausalLM\":\n",
        "                from transformers import LlamaForCausalLM\n",
        "                model = LlamaForCausalLM.from_pretrained(\n",
        "                    str(temp_path),\n",
        "                    local_files_only=True,\n",
        "                    use_safetensors=not force_bin,\n",
        "                )\n",
        "            # Add other model classes as needed\n",
        "        else:\n",
        "            model = AutoModel.from_pretrained(\n",
        "                str(temp_path),\n",
        "                local_files_only=True,\n",
        "                use_safetensors=not force_bin,\n",
        "            )\n",
        "        logger.success(\"Model loaded successfully\")\n",
        "\n",
        "        tokenizer = None\n",
        "        if has_tokenizer_files:\n",
        "            try:\n",
        "                logger.info(\"Loading tokenizer\")\n",
        "                # Special handling for Llama models\n",
        "                if 'LlamaTokenizer' in tokenizer_type or any(f.endswith('tokenizer.model') for f in tokenizer_files):\n",
        "                    logger.debug_log(\"Using LlamaTokenizerFast\")\n",
        "                    from transformers import LlamaTokenizerFast\n",
        "                    tokenizer = LlamaTokenizerFast.from_pretrained(\n",
        "                        str(temp_path),\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                else:\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(\n",
        "                        str(temp_path),\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                logger.success(\"Tokenizer loaded successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load tokenizer: {str(e)}\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "GebJ0DL7miRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with debugging enabled\n",
        "try:\n",
        "    model, tokenizer = load_model_from_s3(\n",
        "        bucket=\"my-bucket\",\n",
        "        path_to_model=\"models/my-model\",\n",
        "        endpoint_url=\"https://my-storage-endpoint\",\n",
        "        verify_ssl=False,\n",
        "        enable_debug=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model: {e}\")"
      ],
      "metadata": {
        "id": "_ChlOgdWJ8ET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}