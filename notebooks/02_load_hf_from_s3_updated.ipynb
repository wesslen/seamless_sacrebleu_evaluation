{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZxRl64WFZ2WDUpGPwlokd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/02_load_hf_from_s3_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install boto3 transformers torch"
      ],
      "metadata": {
        "id": "LvgUESk_Toc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore.config import Config\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from contextlib import contextmanager\n",
        "from io import BytesIO\n",
        "from tempfile import TemporaryDirectory\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    PretrainedConfig,\n",
        "    AutoProcessor,\n",
        "    LlamaTokenizerFast,\n",
        "    LlamaForCausalLM,\n",
        "    SeamlessM4Tv2Model\n",
        ")\n",
        "from typing import Tuple, Optional, Union, List, Dict, Any\n",
        "from pathlib import Path\n",
        "import urllib3\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "class NotebookLogger:\n",
        "    \"\"\"Custom logger for Jupyter notebooks with colored output\"\"\"\n",
        "\n",
        "    COLORS = {\n",
        "        'INFO': '#0066cc',\n",
        "        'DEBUG': '#666666',\n",
        "        'WARNING': '#ff9900',\n",
        "        'ERROR': '#cc0000',\n",
        "        'SUCCESS': '#009933'\n",
        "    }\n",
        "\n",
        "    def __init__(self, enable_debug=False):\n",
        "        self.enable_debug = enable_debug\n",
        "\n",
        "    def _log(self, level: str, message: str):\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        color = self.COLORS.get(level, '#000000')\n",
        "        display(HTML(\n",
        "            f'<pre style=\"margin:0; padding:2px 0; color: {color}\">'\n",
        "            f'[{timestamp}] {level}: {message}'\n",
        "            '</pre>'\n",
        "        ))\n",
        "\n",
        "    def info(self, message: str): self._log('INFO', message)\n",
        "    def debug_log(self, message: str):\n",
        "        if self.enable_debug: self._log('DEBUG', message)\n",
        "    def warning(self, message: str): self._log('WARNING', message)\n",
        "    def error(self, message: str): self._log('ERROR', message)\n",
        "    def success(self, message: str): self._log('SUCCESS', message)\n",
        "\n",
        "def get_s3_client(endpoint_url: Optional[str] = None, verify_ssl: bool = True):\n",
        "    \"\"\"Create an S3 client with configurable SSL verification.\"\"\"\n",
        "    if not verify_ssl:\n",
        "        warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    config = Config(retries=dict(max_attempts=3))\n",
        "    return boto3.client(\n",
        "        's3',\n",
        "        endpoint_url=endpoint_url,\n",
        "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
        "        verify=verify_ssl,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "def find_model_files(files: List[str], logger: NotebookLogger) -> Tuple[List[str], bool]:\n",
        "    \"\"\"Find model files and determine if they're sharded.\"\"\"\n",
        "    # Check for single safetensors file\n",
        "    single_file = [f for f in files if f.endswith('model.safetensors')]\n",
        "    if single_file:\n",
        "        logger.debug_log(\"Found single safetensors file\")\n",
        "        return single_file, False\n",
        "\n",
        "    # Check for sharded files\n",
        "    sharded_pattern = re.compile(r'model-\\d{5}-of-\\d{5}\\.safetensors$')\n",
        "    sharded_files = [f for f in files if sharded_pattern.search(os.path.basename(f))]\n",
        "\n",
        "    if sharded_files:\n",
        "        logger.debug_log(f\"Found {len(sharded_files)} sharded safetensors files\")\n",
        "        return sorted(sharded_files), True\n",
        "\n",
        "    logger.debug_log(\"No safetensors files found\")\n",
        "    return [], False\n",
        "\n",
        "def get_model_class(model_type: str, model_class: Optional[str], logger: NotebookLogger):\n",
        "    \"\"\"\n",
        "    Determine the appropriate model class based on model type and class name.\n",
        "    \"\"\"\n",
        "    if model_class:\n",
        "        if model_class == \"LlamaForCausalLM\":\n",
        "            return LlamaForCausalLM\n",
        "        return AutoModel\n",
        "\n",
        "    MODEL_CLASS_MAPPING = {\n",
        "        \"llama\": LlamaForCausalLM,\n",
        "        \"seamless_m4t_v2\": SeamlessM4Tv2Model,\n",
        "    }\n",
        "\n",
        "    model_class = MODEL_CLASS_MAPPING.get(model_type)\n",
        "    if model_class:\n",
        "        logger.debug_log(f\"Using specific model class: {model_class.__name__}\")\n",
        "        return model_class\n",
        "\n",
        "    logger.debug_log(\"Using default AutoModel class\")\n",
        "    return AutoModel\n",
        "\n",
        "def load_model_from_s3(\n",
        "    bucket: str,\n",
        "    path_to_model: str,\n",
        "    endpoint_url: Optional[str] = None,\n",
        "    verify_ssl: bool = True,\n",
        "    force_bin: bool = False,\n",
        "    enable_debug: bool = False,\n",
        "    model_class: Optional[str] = None\n",
        ") -> Tuple[Union[AutoModel, None], Union[Any, None]]:\n",
        "    \"\"\"\n",
        "    Load a model and its tokenizer/processor from S3 storage.\n",
        "\n",
        "    Args:\n",
        "        bucket: S3 bucket name\n",
        "        path_to_model: Path to model directory in bucket\n",
        "        endpoint_url: Custom S3 endpoint URL\n",
        "        verify_ssl: Whether to verify SSL certificates\n",
        "        force_bin: Force using .bin format even if safetensors available\n",
        "        enable_debug: Enable detailed debug logging\n",
        "        model_class: Specific model class to use (e.g., \"LlamaForCausalLM\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, tokenizer_or_processor)\n",
        "    \"\"\"\n",
        "    logger = NotebookLogger(enable_debug=enable_debug)\n",
        "    logger.info(f\"Starting model load from bucket: {bucket}, path: {path_to_model}\")\n",
        "\n",
        "    s3_client = get_s3_client(endpoint_url, verify_ssl)\n",
        "\n",
        "    # List all files in the model directory\n",
        "    logger.info(\"Listing files in bucket...\")\n",
        "    files = []\n",
        "    paginator = s3_client.get_paginator('list_objects_v2')\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=path_to_model):\n",
        "        if 'Contents' in page:\n",
        "            files.extend(obj['Key'] for obj in page['Contents'])\n",
        "\n",
        "    logger.debug_log(f\"Found {len(files)} total files\")\n",
        "\n",
        "    # Find model files\n",
        "    safetensors_files, is_sharded = find_model_files(files, logger)\n",
        "    bin_files = [f for f in files if f.endswith('.bin')]\n",
        "\n",
        "    with TemporaryDirectory() as temp_dir:\n",
        "        temp_path = Path(temp_dir)\n",
        "        logger.debug_log(f\"Created temporary directory: {temp_dir}\")\n",
        "\n",
        "        # Download and read config\n",
        "        config_file = next((f for f in files if f.endswith('config.json')), None)\n",
        "        if not config_file:\n",
        "            raise Exception(\"No config.json found in model directory\")\n",
        "\n",
        "        config_path = temp_path / 'config.json'\n",
        "        with open(config_path, 'wb') as out:\n",
        "            obj = s3_client.get_object(Bucket=bucket, Key=config_file)\n",
        "            out.write(obj['Body'].read())\n",
        "\n",
        "        with open(config_path) as f:\n",
        "            config_data = json.load(f)\n",
        "            model_type = config_data.get('model_type', '').lower()\n",
        "            logger.debug_log(f\"Detected model type: {model_type}\")\n",
        "\n",
        "        # Handle model weights\n",
        "        if safetensors_files and not force_bin:\n",
        "            logger.info(\"Using safetensors format\")\n",
        "            for file in safetensors_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "\n",
        "            if is_sharded:\n",
        "                # Create index file for sharded safetensors\n",
        "                num_shards = len(safetensors_files)\n",
        "                index_data = {\n",
        "                    \"metadata\": {\"total_size\": 0},\n",
        "                    \"weight_map\": {\n",
        "                        param: f\"model-{i+1:05d}-of-{num_shards:05d}.safetensors\"\n",
        "                        for i, param in enumerate(range(num_shards))\n",
        "                    }\n",
        "                }\n",
        "                with open(temp_path / \"model.safetensors.index.json\", \"w\") as f:\n",
        "                    json.dump(index_data, f)\n",
        "\n",
        "        elif bin_files:\n",
        "            logger.info(\"Using .bin format\")\n",
        "            for file in bin_files:\n",
        "                relative_path = Path(file).relative_to(path_to_model)\n",
        "                target_path = temp_path / relative_path\n",
        "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with open(target_path, 'wb') as out:\n",
        "                    obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                    out.write(obj['Body'].read())\n",
        "        else:\n",
        "            raise Exception(\"No model weights files found\")\n",
        "\n",
        "        # Download all auxiliary files\n",
        "        aux_files = [f for f in files if any(f.endswith(ext) for ext in [\n",
        "            'tokenizer.json',\n",
        "            'tokenizer_config.json',\n",
        "            'special_tokens_map.json',\n",
        "            'vocab.json',\n",
        "            'merges.txt',\n",
        "            'tokenizer.model',\n",
        "            'processor_config.json',\n",
        "            'preprocessor_config.json',\n",
        "            'generation_config.json'\n",
        "        ])]\n",
        "\n",
        "        for file in aux_files:\n",
        "            relative_path = Path(file).relative_to(path_to_model)\n",
        "            target_path = temp_path / relative_path\n",
        "            target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            with open(target_path, 'wb') as out:\n",
        "                obj = s3_client.get_object(Bucket=bucket, Key=file)\n",
        "                out.write(obj['Body'].read())\n",
        "\n",
        "        # Load model\n",
        "        try:\n",
        "            logger.info(\"Loading model...\")\n",
        "            ModelClass = get_model_class(model_type, model_class, logger)\n",
        "\n",
        "            model = ModelClass.from_pretrained(\n",
        "                str(temp_path),\n",
        "                local_files_only=True,\n",
        "                use_safetensors=not force_bin\n",
        "            )\n",
        "            logger.success(\"Model loaded successfully\")\n",
        "\n",
        "            # Load tokenizer or processor\n",
        "            tokenizer_or_processor = None\n",
        "            try:\n",
        "                # For Seamless models, always use AutoProcessor\n",
        "                if model_type == \"seamless_m4t_v2\":\n",
        "                    logger.info(\"Loading Seamless processor...\")\n",
        "                    tokenizer_or_processor = AutoProcessor.from_pretrained(\n",
        "                        str(temp_path),\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    logger.success(\"Seamless processor loaded successfully\")\n",
        "                # Handle Llama models\n",
        "                elif model_type == \"llama\":\n",
        "                    logger.info(\"Loading LlamaTokenizerFast...\")\n",
        "                    tokenizer_or_processor = LlamaTokenizerFast.from_pretrained(\n",
        "                        str(temp_path),\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    logger.success(\"LlamaTokenizerFast loaded successfully\")\n",
        "                # Default to AutoTokenizer\n",
        "                else:\n",
        "                    logger.info(\"Loading tokenizer...\")\n",
        "                    tokenizer_or_processor = AutoTokenizer.from_pretrained(\n",
        "                        str(temp_path),\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    logger.success(\"Tokenizer loaded successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load tokenizer/processor: {str(e)}\")\n",
        "\n",
        "            return model, tokenizer_or_processor\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "BS8xd74ST0VN",
        "outputId": "bd7bf12a-cc0e-44c3-a58a-9f8de1f0295c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'boto3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ec4d024a0521>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Llama model\n",
        "model, tokenizer = load_model_from_s3(\n",
        "    bucket=\"my-bucket\",\n",
        "    path_to_model=\"models/llama-3.1-8b-instruct\",\n",
        "    endpoint_url=\"https://my-storage-endpoint\",\n",
        "    verify_ssl=False,\n",
        "    model_class=\"LlamaForCausalLM\",\n",
        "    enable_debug=True\n",
        ")"
      ],
      "metadata": {
        "id": "_ChlOgdWJ8ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Seamless model\n",
        "model, processor = load_model_from_s3(\n",
        "    bucket=\"my-bucket\",\n",
        "    path_to_model=\"models/seamless-m4t-v2-large\",\n",
        "    endpoint_url=\"https://my-storage-endpoint\",\n",
        "    verify_ssl=False,\n",
        "    enable_debug=True\n",
        ")"
      ],
      "metadata": {
        "id": "nr8pWY5DT1pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model, tokenizer = load_model_from_s3(\n",
        "        bucket=\"my-bucket\",\n",
        "        path_to_model=\"models/my-model\",\n",
        "        endpoint_url=\"https://my-storage-endpoint\",\n",
        "        verify_ssl=False,\n",
        "        enable_debug=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model: {e}\")"
      ],
      "metadata": {
        "id": "Ym10bHeIT30E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}