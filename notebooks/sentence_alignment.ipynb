{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7yBi/xP/u5tEyBXpmxDQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/sentence_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install --system docling"
      ],
      "metadata": {
        "id": "oQuw07aA_pjS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/english-sample.docx\n",
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/spanish-sample.docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0EgfHpvAEee",
        "outputId": "308b83e6-d9a3-4749-e320-fbf6d2ccc8a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 17:21:06--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/english-sample.docx\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/english-sample.docx [following]\n",
            "--2024-12-07 17:21:07--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/english-sample.docx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15752 (15K) [application/octet-stream]\n",
            "Saving to: ‘english-sample.docx.1’\n",
            "\n",
            "english-sample.docx 100%[===================>]  15.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-07 17:21:07 (43.9 MB/s) - ‘english-sample.docx.1’ saved [15752/15752]\n",
            "\n",
            "--2024-12-07 17:21:07--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/spanish-sample.docx\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/spanish-sample.docx [following]\n",
            "--2024-12-07 17:21:07--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/spanish-sample.docx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16147 (16K) [application/octet-stream]\n",
            "Saving to: ‘spanish-sample.docx.1’\n",
            "\n",
            "spanish-sample.docx 100%[===================>]  15.77K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-12-07 17:21:07 (27.5 MB/s) - ‘spanish-sample.docx.1’ saved [16147/16147]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "def convert_to_markdown(document_path):\n",
        "  \"\"\"Converts a document to markdown format.\n",
        "\n",
        "  Args:\n",
        "    document_path: The path to the document.\n",
        "\n",
        "  Returns:\n",
        "    The markdown representation of the document.\n",
        "  \"\"\"\n",
        "  converter = DocumentConverter()\n",
        "  result = converter.convert(document_path)\n",
        "  return result.document.export_to_markdown()\n",
        "\n",
        "# Example usage\n",
        "english = convert_to_markdown(\"english-sample.docx\")\n",
        "spanish = convert_to_markdown(\"spanish-sample.docx\")"
      ],
      "metadata": {
        "id": "6Kf8-2GC_s_d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save english as english.txt\n",
        "with open('english.txt', 'w') as f:\n",
        "    f.write(english)\n",
        "\n",
        "# save spanish as spanish.txt\n",
        "with open('spanish.txt', 'w') as f:\n",
        "    f.write(spanish)"
      ],
      "metadata": {
        "id": "ceqH1O4TpVT8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import chardet\n",
        "import spacy\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "class FileValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class SentencePair:\n",
        "    source: str\n",
        "    target: str\n",
        "    source_index: List[int]\n",
        "    target_index: List[int]\n",
        "    alignment_score: float = 0.0\n",
        "\n",
        "class GaleChurchAligner:\n",
        "    # Constants for Gale-Church algorithm\n",
        "    MEAN_CHARACTERS_RATIO = 1\n",
        "    VARIANCE_CHARACTERS_RATIO = 6.8\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Gale-Church Aligner...\")\n",
        "        self.log_prob_tables = {}\n",
        "\n",
        "    def char_length_ratio(self, source_len: int, target_len: int) -> float:\n",
        "        try:\n",
        "            ratio = (target_len - source_len * self.MEAN_CHARACTERS_RATIO) / \\\n",
        "                    math.sqrt(source_len * self.VARIANCE_CHARACTERS_RATIO)\n",
        "            return -math.log(1 + ratio * ratio)\n",
        "        except (ValueError, ZeroDivisionError):\n",
        "            return float('-inf')\n",
        "\n",
        "    def align_blocks(\n",
        "        self,\n",
        "        source_sents: List[str],\n",
        "        target_sents: List[str]\n",
        "    ) -> List[Tuple[List[int], List[int], float]]:\n",
        "        print(f\"Starting alignment of {len(source_sents)} source and {len(target_sents)} target sentences...\")\n",
        "\n",
        "        n, m = len(source_sents), len(target_sents)\n",
        "\n",
        "        # Initialize DP tables\n",
        "        dp = defaultdict(lambda: float('inf'))\n",
        "        dp[0, 0] = 0\n",
        "        back = {}\n",
        "\n",
        "        # Alignment patterns (1-1, 1-2, 2-1, 2-2)\n",
        "        patterns = [(1,1), (1,2), (2,1), (2,2)]\n",
        "\n",
        "        # Progress tracking\n",
        "        total_steps = (n + 1) * (m + 1)\n",
        "        current_step = 0\n",
        "\n",
        "        print(\"Computing optimal alignments...\")\n",
        "        # Fill DP table\n",
        "        for i in range(n + 1):\n",
        "            for j in range(m + 1):\n",
        "                current_step += 1\n",
        "                if current_step % 100 == 0:\n",
        "                    print(f\"Progress: {current_step}/{total_steps} steps ({(current_step/total_steps)*100:.1f}%)\")\n",
        "\n",
        "                if i == 0 and j == 0:\n",
        "                    continue\n",
        "\n",
        "                for si, ti in patterns:\n",
        "                    if i >= si and j >= ti:\n",
        "                        source_block = source_sents[i-si:i]\n",
        "                        target_block = target_sents[j-ti:j]\n",
        "                        source_len = sum(len(s) for s in source_block)\n",
        "                        target_len = sum(len(t) for t in target_block)\n",
        "\n",
        "                        if source_len and target_len:\n",
        "                            cost = -self.char_length_ratio(source_len, target_len)\n",
        "                            if dp[i-si, j-ti] + cost < dp[i, j]:\n",
        "                                dp[i, j] = dp[i-si, j-ti] + cost\n",
        "                                back[i, j] = (si, ti)\n",
        "\n",
        "        print(\"Reconstructing alignments...\")\n",
        "        alignments = []\n",
        "        i, j = n, m\n",
        "        while i > 0 or j > 0:\n",
        "            si, ti = back.get((i, j), (1, 1))\n",
        "            source_indices = list(range(i-si, i))\n",
        "            target_indices = list(range(j-ti, j))\n",
        "            score = dp[i, j] - dp[i-si, j-ti]\n",
        "            alignments.append((source_indices, target_indices, score))\n",
        "            i, j = i-si, j-ti\n",
        "\n",
        "        print(f\"Found {len(alignments)} alignments\")\n",
        "        return list(reversed(alignments))\n",
        "\n",
        "class SentenceAligner:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Sentence Aligner...\")\n",
        "        # Create blank spaCy models for both languages\n",
        "        self.source_nlp = spacy.blank(\"es\")  # Spanish\n",
        "        self.target_nlp = spacy.blank(\"en\")  # English\n",
        "\n",
        "        # Add the sentencizer to both models\n",
        "        self.source_nlp.add_pipe(\"sentencizer\")\n",
        "        self.target_nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "        # Common abbreviations that shouldn't split sentences\n",
        "        self.abbreviations = {\n",
        "            \"Dr.\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Jr.\", \"Sr.\", \"Inc.\", \"Ltd.\", \"Co.\",\n",
        "            \"St.\", \"Ave.\", \"Blvd.\", \"Rd.\", \"etc.\", \"vs.\", \"tel.\", \"div.\",\n",
        "            \"Vol.\", \"Prof.\", \"Ph.D.\", \"M.D.\", \"B.A.\", \"M.A.\",\n",
        "        }\n",
        "\n",
        "        self.gale_church = GaleChurchAligner()\n",
        "        print(\"Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_encoding(file_path: Path) -> str:\n",
        "        \"\"\"Detect the encoding of a file.\"\"\"\n",
        "        print(f\"Detecting encoding for: {file_path}\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        print(f\"Detected encoding: {result['encoding']} (confidence: {result['confidence']:.2f})\")\n",
        "        return result['encoding']\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_file_contents(text: str) -> bool:\n",
        "        \"\"\"Validate the contents of a file.\"\"\"\n",
        "        print(\"Validating file contents...\")\n",
        "        if not text.strip():\n",
        "            raise FileValidationError(\"File is empty or contains only whitespace\")\n",
        "        if len(text) > 10_000_000:  # 10MB text limit\n",
        "            raise FileValidationError(\"File exceeds size limit\")\n",
        "        print(\"File validation successful\")\n",
        "        return True\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean text while preserving important whitespace.\"\"\"\n",
        "        # Replace multiple spaces with single space\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # Remove spaces before punctuation\n",
        "        text = re.sub(r' ([.,!?])', r'\\1', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def protect_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Replace periods in known abbreviations with a special marker.\"\"\"\n",
        "        protected_text = text\n",
        "        for abbr in self.abbreviations:\n",
        "            # Use word boundaries to avoid partial matches\n",
        "            protected_text = re.sub(\n",
        "                r'\\b' + re.escape(abbr) + r'\\b',\n",
        "                lambda m: m.group().replace('.', '@POINT@'),\n",
        "                protected_text\n",
        "            )\n",
        "        return protected_text\n",
        "\n",
        "    def restore_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Restore the original periods in abbreviations.\"\"\"\n",
        "        return text.replace('@POINT@', '.')\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Apply preprocessing rules to text.\"\"\"\n",
        "        # First protect abbreviations\n",
        "        text = self.protect_abbreviations(text)\n",
        "\n",
        "        # Split into lines and process\n",
        "        lines = text.split('\\n')\n",
        "        processed_lines = []\n",
        "\n",
        "        in_address_block = False\n",
        "        in_list = False\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                in_address_block = False\n",
        "                in_list = False\n",
        "                processed_lines.append('')\n",
        "                continue\n",
        "\n",
        "            # Detect address blocks\n",
        "            if re.search(r'([A-Z]{2}\\s+\\d{5})|(\\(\\d{3}\\)\\s*\\d{3}-\\d{4})|(\\d+\\s+[A-Za-z]+\\s+(Street|Ave|Avenue|Road|Rd|Boulevard|Blvd))', line):\n",
        "                in_address_block = True\n",
        "\n",
        "            # Detect list items\n",
        "            if line.startswith(('-', '•', '*')) or re.match(r'^\\d+\\.', line):\n",
        "                in_list = True\n",
        "\n",
        "            # Keep newlines for address blocks and lists\n",
        "            if in_address_block or in_list:\n",
        "                processed_lines.append(line + '\\n')\n",
        "            else:\n",
        "                # For regular text, only add space if not empty\n",
        "                if processed_lines and processed_lines[-1]:\n",
        "                    processed_lines[-1] = processed_lines[-1].rstrip() + ' ' + line\n",
        "                else:\n",
        "                    processed_lines.append(line)\n",
        "\n",
        "        processed_text = '\\n'.join(processed_lines)\n",
        "        # Restore abbreviations before returning\n",
        "        return self.restore_abbreviations(processed_text)\n",
        "\n",
        "    def split_into_blocks(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into logical blocks while preserving structure.\"\"\"\n",
        "        # First protect abbreviations\n",
        "        text = self.protect_abbreviations(text)\n",
        "\n",
        "        blocks = []\n",
        "        current_block = []\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Start a new block if:\n",
        "            # 1. Empty line\n",
        "            # 2. Line starts with a date pattern\n",
        "            # 3. Line is part of an address block\n",
        "            # 4. Line is a salutation or closing\n",
        "            if (not line or\n",
        "                re.match(r'^\\d{1,2}\\s+de\\s+[A-Za-zá-úÁ-Ú]+\\s+de\\s+\\d{4}$|^\\w+\\s+\\d{1,2},\\s+\\d{4}$', line) or\n",
        "                re.match(r'^[A-Za-z0-9\\s]+,\\s*[A-Z]{2}\\s+\\d{5}$', line) or\n",
        "                re.match(r'^(Dear|Estimado|Sincerely|Atentamente)', line)):\n",
        "\n",
        "                if current_block:\n",
        "                    block_text = ' '.join(current_block)\n",
        "                    blocks.append(self.restore_abbreviations(block_text))\n",
        "                    current_block = []\n",
        "                if line:\n",
        "                    blocks.append(self.restore_abbreviations(line))\n",
        "            else:\n",
        "                current_block.append(line)\n",
        "\n",
        "        if current_block:\n",
        "            block_text = ' '.join(current_block)\n",
        "            blocks.append(self.restore_abbreviations(block_text))\n",
        "\n",
        "        return blocks\n",
        "\n",
        "    def tokenize_sentences(self, text: str, is_source: bool = True) -> List[str]:\n",
        "        \"\"\"Split text into sentences using spaCy while preserving structure.\"\"\"\n",
        "        print(\"\\nTokenizing sentences...\")\n",
        "\n",
        "        # Preprocess the text\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        # Split into blocks first\n",
        "        blocks = self.split_into_blocks(text)\n",
        "\n",
        "        # Process each block\n",
        "        sentences = []\n",
        "        nlp = self.source_nlp if is_source else self.target_nlp\n",
        "\n",
        "        for block in blocks:\n",
        "            # If block is a structural element (date, address, etc.), keep it as is\n",
        "            if (re.match(r'^\\d{1,2}\\s+de\\s+[A-Za-zá-úÁ-Ú]+\\s+de\\s+\\d{4}$|^\\w+\\s+\\d{1,2},\\s+\\d{4}$', block) or\n",
        "                re.match(r'^[A-Za-z0-9\\s]+,\\s*[A-Z]{2}\\s+\\d{5}$', block) or\n",
        "                re.match(r'^(Dear|Estimado|Sincerely|Atentamente)', block) or\n",
        "                re.match(r'^[-•*]\\s+', block)):  # List items\n",
        "                sentences.append(block)\n",
        "            else:\n",
        "                # Use spaCy for sentence tokenization\n",
        "                doc = nlp(block)\n",
        "                block_sentences = [str(sent).strip() for sent in doc.sents]\n",
        "                sentences.extend(block_sentences)\n",
        "\n",
        "        print(f\"Found {len(sentences)} sentences\")\n",
        "        if sentences:\n",
        "            print(\"\\nFirst few sentences found:\")\n",
        "            for i, sent in enumerate(sentences[:3]):\n",
        "                print(f\"{i+1}. {sent}\")\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def is_valid_sentence(self, sentence: str) -> bool:\n",
        "        \"\"\"Enhanced validation for sentences and structural elements.\"\"\"\n",
        "        # Allow structural elements to pass through\n",
        "        if (re.match(r'^\\d{1,2}\\s+de\\s+[A-Za-zá-úÁ-Ú]+\\s+de\\s+\\d{4}$|^\\w+\\s+\\d{1,2},\\s+\\d{4}$', sentence) or\n",
        "            re.match(r'^[A-Za-z0-9\\s]+,\\s*[A-Z]{2}\\s+\\d{5}$', sentence) or\n",
        "            re.match(r'^(Dear|Estimado|Sincerely|Atentamente)', sentence) or\n",
        "            re.match(r'^\\s*[-•*]\\s+', sentence)):  # List items\n",
        "            return True\n",
        "\n",
        "        # Original validation for regular sentences\n",
        "        sentence = self.clean_text(sentence)\n",
        "        words = sentence.split()\n",
        "\n",
        "        if len(words) <= 1 or len(words) > 100:  # Increased max length\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            alphanumeric_chars = sum(c.isalnum() for c in sentence)\n",
        "            if alphanumeric_chars / len(sentence) < 0.01:\n",
        "                return False\n",
        "        except ZeroDivisionError:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def align_sentences(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        target_text: str\n",
        "    ) -> Tuple[List[Dict], List[Dict]]:\n",
        "        \"\"\"Align sentences between source and target texts.\"\"\"\n",
        "        try:\n",
        "            print(\"\\nStarting sentence alignment process...\")\n",
        "\n",
        "            # Tokenize using appropriate language models\n",
        "            source_sentences = self.tokenize_sentences(source_text, is_source=True)\n",
        "            target_sentences = self.tokenize_sentences(target_text, is_source=False)\n",
        "\n",
        "            alignments = self.gale_church.align_blocks(source_sentences, target_sentences)\n",
        "\n",
        "            aligned_pairs = []\n",
        "            excluded_pairs = []\n",
        "\n",
        "            print(\"\\nProcessing aligned pairs...\")\n",
        "            for source_indices, target_indices, score in alignments:\n",
        "                source_block = [source_sentences[i] for i in source_indices]\n",
        "                target_block = [target_sentences[i] for i in target_indices]\n",
        "\n",
        "                pair_dict = {\n",
        "                    \"source\": \" \".join(source_block),\n",
        "                    \"target\": \" \".join(target_block),\n",
        "                    \"source_index\": source_indices,\n",
        "                    \"target_index\": target_indices,\n",
        "                    \"alignment_score\": score\n",
        "                }\n",
        "\n",
        "                should_exclude = any(\n",
        "                    not self.is_valid_sentence(sent)\n",
        "                    for sent in source_block + target_block\n",
        "                )\n",
        "\n",
        "                if should_exclude:\n",
        "                    excluded_pairs.append(pair_dict)\n",
        "                else:\n",
        "                    aligned_pairs.append(pair_dict)\n",
        "\n",
        "            print(f\"\\nAlignment complete:\")\n",
        "            print(f\"- Aligned pairs: {len(aligned_pairs)}\")\n",
        "            print(f\"- Excluded pairs: {len(excluded_pairs)}\")\n",
        "\n",
        "            return aligned_pairs, excluded_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sentence alignment: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def process_files(source_path: str, target_path: str, output_dir: str) -> Tuple[Path, Path]:\n",
        "    \"\"\"Process source and target files with detailed progress output.\"\"\"\n",
        "    try:\n",
        "        print(\"\\n=== Starting File Processing ===\")\n",
        "\n",
        "        # Convert to Path objects\n",
        "        source_path = Path(source_path)\n",
        "        target_path = Path(target_path)\n",
        "        output_dir = Path(output_dir)\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "        # Detect and read files\n",
        "        source_encoding = SentenceAligner.detect_encoding(source_path)\n",
        "        target_encoding = SentenceAligner.detect_encoding(target_path)\n",
        "\n",
        "        print(\"\\nReading input files...\")\n",
        "        with open(source_path, 'r', encoding=source_encoding) as f:\n",
        "            source_text = f.read()\n",
        "            print(f\"Read source file: {len(source_text):,} characters\")\n",
        "\n",
        "        with open(target_path, 'r', encoding=target_encoding) as f:\n",
        "            target_text = f.read()\n",
        "            print(f\"Read target file: {len(target_text):,} characters\")\n",
        "\n",
        "        # Validate contents\n",
        "        SentenceAligner.validate_file_contents(source_text)\n",
        "        SentenceAligner.validate_file_contents(target_text)\n",
        "\n",
        "        # Process texts\n",
        "        aligner = SentenceAligner()\n",
        "        aligned_pairs, excluded_pairs = aligner.align_sentences(source_text, target_text)\n",
        "\n",
        "        # Write output files\n",
        "        print(\"\\nWriting output files...\")\n",
        "        aligned_path = output_dir / 'aligned.jsonl'\n",
        "        excluded_path = output_dir / 'excluded.jsonl'\n",
        "\n",
        "        with open(aligned_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in aligned_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "        print(f\"Written {len(aligned_pairs)} pairs to {aligned_path}\")\n",
        "\n",
        "        with open(excluded_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in excluded_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "        print(f\"Written {len(excluded_pairs)} pairs to {excluded_path}\")\n",
        "\n",
        "        print(\"\\n=== File Processing Complete ===\")\n",
        "        return aligned_path, excluded_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Example usage in Jupyter notebook\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Replace these with your actual file paths\n",
        "        source_file = \"spanish.txt\"\n",
        "        target_file = \"english.txt\"\n",
        "        output_directory = \"output\"\n",
        "\n",
        "        print(f\"\\nProcessing files:\")\n",
        "        print(f\"Source: {source_file}\")\n",
        "        print(f\"Target: {target_file}\")\n",
        "        print(f\"Output: {output_directory}\")\n",
        "\n",
        "        aligned_file, excluded_file = process_files(\n",
        "            source_file,\n",
        "            target_file,\n",
        "            output_directory\n",
        "        )\n",
        "\n",
        "        print(\"\\nSuccess!\")\n",
        "        print(f\"Aligned sentences: {aligned_file}\")\n",
        "        print(f\"Excluded sentences: {excluded_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFailed to process files: {str(e)}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlUmsaiJlgqk",
        "outputId": "076545a4-6aa1-40ba-8f5a-81ff081563dc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing files:\n",
            "Source: spanish.txt\n",
            "Target: english.txt\n",
            "Output: output\n",
            "\n",
            "=== Starting File Processing ===\n",
            "Created output directory: output\n",
            "Detecting encoding for: spanish.txt\n",
            "Detected encoding: utf-8 (confidence: 0.99)\n",
            "Detecting encoding for: english.txt\n",
            "Detected encoding: ascii (confidence: 1.00)\n",
            "\n",
            "Reading input files...\n",
            "Read source file: 1,167 characters\n",
            "Read target file: 1,060 characters\n",
            "Validating file contents...\n",
            "File validation successful\n",
            "Validating file contents...\n",
            "File validation successful\n",
            "Initializing Sentence Aligner...\n",
            "Initializing Gale-Church Aligner...\n",
            "Initialization complete\n",
            "\n",
            "Starting sentence alignment process...\n",
            "\n",
            "Tokenizing sentences...\n",
            "Found 21 sentences\n",
            "\n",
            "First few sentences found:\n",
            "1. 15 de marzo de 2024\n",
            "2. Chase Bank 1234 Avenida Financial\n",
            "3. Chicago, IL 60601\n",
            "\n",
            "Tokenizing sentences...\n",
            "Found 20 sentences\n",
            "\n",
            "First few sentences found:\n",
            "1. March 15, 2024\n",
            "2. Chase Bank 1234 Financial Avenue Chicago, IL 60601\n",
            "3. Dear Sir/Madam,\n",
            "Starting alignment of 21 source and 20 target sentences...\n",
            "Computing optimal alignments...\n",
            "Progress: 100/462 steps (21.6%)\n",
            "Progress: 200/462 steps (43.3%)\n",
            "Progress: 300/462 steps (64.9%)\n",
            "Progress: 400/462 steps (86.6%)\n",
            "Reconstructing alignments...\n",
            "Found 13 alignments\n",
            "\n",
            "Processing aligned pairs...\n",
            "\n",
            "Alignment complete:\n",
            "- Aligned pairs: 12\n",
            "- Excluded pairs: 1\n",
            "\n",
            "Writing output files...\n",
            "Written 12 pairs to output/aligned.jsonl\n",
            "Written 1 pairs to output/excluded.jsonl\n",
            "\n",
            "=== File Processing Complete ===\n",
            "\n",
            "Success!\n",
            "Aligned sentences: output/aligned.jsonl\n",
            "Excluded sentences: output/excluded.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "def convert_alignment_to_translation(input_path: str, output_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Convert aligned sentence pairs from Gale-Church format to translation evaluation format.\n",
        "\n",
        "    Args:\n",
        "        input_path: Path to the aligned.jsonl file\n",
        "        output_path: Path to save the converted translation format\n",
        "\n",
        "    The function converts from:\n",
        "        {\"source\": \"text\", \"target\": \"text\", \"source_index\": [0], \"target_index\": [0], \"alignment_score\": 0.5}\n",
        "    To:\n",
        "        {\"source_text\": \"text\", \"references\": [\"text\"]}\n",
        "    \"\"\"\n",
        "    print(f\"Converting alignment file: {input_path}\")\n",
        "\n",
        "    # Ensure input file exists\n",
        "    if not Path(input_path).exists():\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
        "\n",
        "    translations = []\n",
        "\n",
        "    # Read and convert alignments\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            alignment = json.loads(line)\n",
        "            translation = {\n",
        "                \"source_text\": alignment[\"source\"],\n",
        "                \"references\": [alignment[\"target\"]]\n",
        "            }\n",
        "            translations.append(translation)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_path = Path(output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Write translations\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for translation in translations:\n",
        "            json.dump(translation, f, ensure_ascii=False)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"Conversion complete. Processed {len(translations)} pairs.\")\n",
        "    print(f\"Output written to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "convert_alignment_to_translation(\n",
        "    input_path=\"output/aligned.jsonl\",\n",
        "    output_path=\"output/translations.jsonl\"\n",
        ")"
      ],
      "metadata": {
        "id": "NV7sYAAVFUxy",
        "outputId": "f49b4898-ab27-422b-b9e4-a02051ddff64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting alignment file: output/aligned.jsonl\n",
            "Conversion complete. Processed 12 pairs.\n",
            "Output written to: output/translations.jsonl\n"
          ]
        }
      ]
    }
  ]
}