{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPboWe/M1WK+5tydBaKLI75",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebook/sentence_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english = \"\"\"\n",
        "Machine learning is transforming the way we interact with technology. It powers everything from recommendation systems to autonomous vehicles.\n",
        "\n",
        "## Basic Concepts\n",
        "\n",
        "Neural networks are inspired by the human brain! They consist of interconnected nodes that process information in layers.\n",
        "\n",
        "### Types of Learning\n",
        "\n",
        "Supervised learning requires labeled data.   Multiple spaces    here should be cleaned.\n",
        "\n",
        "Unsupervised learning finds patterns without labels.\n",
        "\n",
        "* This is a bullet point.\n",
        "* x\n",
        "\n",
        "Semi-supervised learning combines both approaches...\n",
        "\n",
        "## Advanced Topics\n",
        "\n",
        "Deep learning has revolutionized computer vision and natural language processing.\n",
        "\n",
        "Transfer learning allows models to apply knowledge from one domain to another.\n",
        "\n",
        "Contact: info@example.com\n",
        "\"\"\"\n",
        "\n",
        "spanish = \"\"\"\n",
        "El aprendizaje automático está transformando la forma en que interactuamos con la tecnología. Impulsa todo, desde sistemas de recomendación hasta vehículos autónomos.\n",
        "\n",
        "## Conceptos Básicos\n",
        "\n",
        "¡Las redes neuronales están inspiradas en el cerebro humano! Consisten en nodos interconectados que procesan información en capas.\n",
        "\n",
        "### Tipos de Aprendizaje\n",
        "\n",
        "El aprendizaje supervisado requiere datos etiquetados.    Múltiples espacios    aquí deben limpiarse.\n",
        "\n",
        "El aprendizaje no supervisado encuentra patrones sin etiquetas.\n",
        "\n",
        "* Este es un punto de viñeta.\n",
        "* x\n",
        "\n",
        "El aprendizaje semisupervisado combina ambos enfoques...\n",
        "\n",
        "## Temas Avanzados\n",
        "\n",
        "El aprendizaje profundo ha revolucionado la visión por computadora y el procesamiento del lenguaje natural.\n",
        "\n",
        "La transferencia de aprendizaje permite que los modelos apliquen el conocimiento de un dominio a otro.\n",
        "\n",
        "Contacto: info@example.com\n",
        "\"\"\"\n",
        "\n",
        "# save english as english.txt\n",
        "with open('english.txt', 'w') as f:\n",
        "    f.write(english)\n",
        "\n",
        "# save spanish as spanish.txt\n",
        "with open('spanish.txt', 'w') as f:\n",
        "    f.write(spanish)"
      ],
      "metadata": {
        "id": "ceqH1O4TpVT8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import chardet\n",
        "import spacy\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "class FileValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class SentencePair:\n",
        "    source: str\n",
        "    target: str\n",
        "    source_index: List[int]\n",
        "    target_index: List[int]\n",
        "    alignment_score: float = 0.0\n",
        "\n",
        "class GaleChurchAligner:\n",
        "    # Constants for Gale-Church algorithm\n",
        "    MEAN_CHARACTERS_RATIO = 1\n",
        "    VARIANCE_CHARACTERS_RATIO = 6.8\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Gale-Church Aligner...\")\n",
        "        self.log_prob_tables = {}\n",
        "\n",
        "    def char_length_ratio(self, source_len: int, target_len: int) -> float:\n",
        "        try:\n",
        "            ratio = (target_len - source_len * self.MEAN_CHARACTERS_RATIO) / \\\n",
        "                    math.sqrt(source_len * self.VARIANCE_CHARACTERS_RATIO)\n",
        "            return -math.log(1 + ratio * ratio)\n",
        "        except (ValueError, ZeroDivisionError):\n",
        "            return float('-inf')\n",
        "\n",
        "    def align_blocks(\n",
        "        self,\n",
        "        source_sents: List[str],\n",
        "        target_sents: List[str]\n",
        "    ) -> List[Tuple[List[int], List[int], float]]:\n",
        "        print(f\"Starting alignment of {len(source_sents)} source and {len(target_sents)} target sentences...\")\n",
        "\n",
        "        n, m = len(source_sents), len(target_sents)\n",
        "\n",
        "        # Initialize DP tables\n",
        "        dp = defaultdict(lambda: float('inf'))\n",
        "        dp[0, 0] = 0\n",
        "        back = {}\n",
        "\n",
        "        # Alignment patterns (1-1, 1-2, 2-1, 2-2)\n",
        "        patterns = [(1,1), (1,2), (2,1), (2,2)]\n",
        "\n",
        "        # Progress tracking\n",
        "        total_steps = (n + 1) * (m + 1)\n",
        "        current_step = 0\n",
        "\n",
        "        print(\"Computing optimal alignments...\")\n",
        "        # Fill DP table\n",
        "        for i in range(n + 1):\n",
        "            for j in range(m + 1):\n",
        "                current_step += 1\n",
        "                if current_step % 100 == 0:\n",
        "                    print(f\"Progress: {current_step}/{total_steps} steps ({(current_step/total_steps)*100:.1f}%)\")\n",
        "\n",
        "                if i == 0 and j == 0:\n",
        "                    continue\n",
        "\n",
        "                for si, ti in patterns:\n",
        "                    if i >= si and j >= ti:\n",
        "                        source_block = source_sents[i-si:i]\n",
        "                        target_block = target_sents[j-ti:j]\n",
        "                        source_len = sum(len(s) for s in source_block)\n",
        "                        target_len = sum(len(t) for t in target_block)\n",
        "\n",
        "                        if source_len and target_len:\n",
        "                            cost = -self.char_length_ratio(source_len, target_len)\n",
        "                            if dp[i-si, j-ti] + cost < dp[i, j]:\n",
        "                                dp[i, j] = dp[i-si, j-ti] + cost\n",
        "                                back[i, j] = (si, ti)\n",
        "\n",
        "        print(\"Reconstructing alignments...\")\n",
        "        alignments = []\n",
        "        i, j = n, m\n",
        "        while i > 0 or j > 0:\n",
        "            si, ti = back.get((i, j), (1, 1))\n",
        "            source_indices = list(range(i-si, i))\n",
        "            target_indices = list(range(j-ti, j))\n",
        "            score = dp[i, j] - dp[i-si, j-ti]\n",
        "            alignments.append((source_indices, target_indices, score))\n",
        "            i, j = i-si, j-ti\n",
        "\n",
        "        print(f\"Found {len(alignments)} alignments\")\n",
        "        return list(reversed(alignments))\n",
        "\n",
        "class SentenceAligner:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Sentence Aligner...\")\n",
        "        # Create blank spaCy models for both languages\n",
        "        self.source_nlp = spacy.blank(\"es\")  # Spanish\n",
        "        self.target_nlp = spacy.blank(\"en\")  # English\n",
        "\n",
        "        # Add the sentencizer to both models\n",
        "        self.source_nlp.add_pipe(\"sentencizer\")\n",
        "        self.target_nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "        self.gale_church = GaleChurchAligner()\n",
        "        print(\"Initialization complete\")\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_encoding(file_path: Path) -> str:\n",
        "        print(f\"Detecting encoding for: {file_path}\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        print(f\"Detected encoding: {result['encoding']} (confidence: {result['confidence']:.2f})\")\n",
        "        return result['encoding']\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_file_contents(text: str) -> bool:\n",
        "        print(\"Validating file contents...\")\n",
        "        if not text.strip():\n",
        "            raise FileValidationError(\"File is empty or contains only whitespace\")\n",
        "        if len(text) > 10_000_000:\n",
        "            raise FileValidationError(\"File exceeds size limit\")\n",
        "        print(\"File validation successful\")\n",
        "        return True\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Apply whitespace cleaning rules to text.\"\"\"\n",
        "        # Replace multiple whitespace characters with single space\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def is_valid_sentence(self, sentence: str) -> bool:\n",
        "        \"\"\"Check if sentence meets inclusion criteria.\"\"\"\n",
        "        sentence = self.clean_text(sentence)\n",
        "\n",
        "        words = sentence.split()\n",
        "        if len(words) <= 1 or len(words) > 50:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            alphanumeric_chars = sum(c.isalnum() for c in sentence)\n",
        "            if alphanumeric_chars / len(sentence) < 0.01:\n",
        "                return False\n",
        "        except ZeroDivisionError:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def tokenize_sentences(self, text: str, is_source: bool = True) -> List[str]:\n",
        "        \"\"\"Split text into sentences using spaCy.\"\"\"\n",
        "        print(\"\\nTokenizing sentences...\")\n",
        "        print(f\"Input text length: {len(text)} characters\")\n",
        "\n",
        "        # Clean the text\n",
        "        text = self.clean_text(text)\n",
        "        print(f\"Cleaned text length: {len(text)} characters\")\n",
        "\n",
        "        # Use appropriate model based on source/target\n",
        "        nlp = self.source_nlp if is_source else self.target_nlp\n",
        "\n",
        "        # Process the text and get sentences\n",
        "        doc = nlp(text)\n",
        "        sentences = [str(sent).strip() for sent in doc.sents]\n",
        "\n",
        "        # Debug output\n",
        "        print(f\"Found {len(sentences)} sentences\")\n",
        "        if sentences:\n",
        "            print(\"\\nFirst few sentences found:\")\n",
        "            for i, sent in enumerate(sentences[:3]):\n",
        "                print(f\"{i+1}. {sent}\")\n",
        "        else:\n",
        "            print(\"WARNING: No sentences were found!\")\n",
        "            print(\"Text sample:\", text[:100], \"...\")\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def align_sentences(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        target_text: str\n",
        "    ) -> Tuple[List[Dict], List[Dict]]:\n",
        "        try:\n",
        "            print(\"\\nStarting sentence alignment process...\")\n",
        "\n",
        "            # Tokenize using appropriate language models\n",
        "            source_sentences = self.tokenize_sentences(source_text, is_source=True)\n",
        "            target_sentences = self.tokenize_sentences(target_text, is_source=False)\n",
        "\n",
        "            alignments = self.gale_church.align_blocks(source_sentences, target_sentences)\n",
        "\n",
        "            aligned_pairs = []\n",
        "            excluded_pairs = []\n",
        "\n",
        "            print(\"\\nProcessing aligned pairs...\")\n",
        "            for source_indices, target_indices, score in alignments:\n",
        "                source_block = [source_sentences[i] for i in source_indices]\n",
        "                target_block = [target_sentences[i] for i in target_indices]\n",
        "\n",
        "                pair_dict = {\n",
        "                    \"source\": \" \".join(source_block),\n",
        "                    \"target\": \" \".join(target_block),\n",
        "                    \"source_index\": source_indices,\n",
        "                    \"target_index\": target_indices,\n",
        "                    \"alignment_score\": score\n",
        "                }\n",
        "\n",
        "                should_exclude = any(\n",
        "                    not self.is_valid_sentence(sent)\n",
        "                    for sent in source_block + target_block\n",
        "                )\n",
        "\n",
        "                if should_exclude:\n",
        "                    excluded_pairs.append(pair_dict)\n",
        "                else:\n",
        "                    aligned_pairs.append(pair_dict)\n",
        "\n",
        "            print(f\"\\nAlignment complete:\")\n",
        "            print(f\"- Aligned pairs: {len(aligned_pairs)}\")\n",
        "            print(f\"- Excluded pairs: {len(excluded_pairs)}\")\n",
        "\n",
        "            return aligned_pairs, excluded_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sentence alignment: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def process_files(source_path: str, target_path: str, output_dir: str) -> Tuple[Path, Path]:\n",
        "    \"\"\"Process source and target files with detailed progress output.\"\"\"\n",
        "    try:\n",
        "        print(\"\\n=== Starting File Processing ===\")\n",
        "\n",
        "        # Convert to Path objects\n",
        "        source_path = Path(source_path)\n",
        "        target_path = Path(target_path)\n",
        "        output_dir = Path(output_dir)\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "        # Detect and read files\n",
        "        source_encoding = SentenceAligner.detect_encoding(source_path)\n",
        "        target_encoding = SentenceAligner.detect_encoding(target_path)\n",
        "\n",
        "        print(\"\\nReading input files...\")\n",
        "        with open(source_path, 'r', encoding=source_encoding) as f:\n",
        "            source_text = f.read()\n",
        "            print(f\"Read source file: {len(source_text):,} characters\")\n",
        "\n",
        "        with open(target_path, 'r', encoding=target_encoding) as f:\n",
        "            target_text = f.read()\n",
        "            print(f\"Read target file: {len(target_text):,} characters\")\n",
        "\n",
        "        # Validate contents\n",
        "        SentenceAligner.validate_file_contents(source_text)\n",
        "        SentenceAligner.validate_file_contents(target_text)\n",
        "\n",
        "        # Process texts\n",
        "        aligner = SentenceAligner()\n",
        "        aligned_pairs, excluded_pairs = aligner.align_sentences(source_text, target_text)\n",
        "\n",
        "        # Write output files\n",
        "        print(\"\\nWriting output files...\")\n",
        "        aligned_path = output_dir / 'aligned.jsonl'\n",
        "        excluded_path = output_dir / 'excluded.jsonl'\n",
        "\n",
        "        with open(aligned_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in aligned_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "        print(f\"Written {len(aligned_pairs)} pairs to {aligned_path}\")\n",
        "\n",
        "        with open(excluded_path, 'w', encoding='utf-8') as f:\n",
        "            for pair in excluded_pairs:\n",
        "                f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
        "        print(f\"Written {len(excluded_pairs)} pairs to {excluded_path}\")\n",
        "\n",
        "        print(\"\\n=== File Processing Complete ===\")\n",
        "        return aligned_path, excluded_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Example usage in Jupyter notebook\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Replace these with your actual file paths\n",
        "        source_file = \"spanish.txt\"\n",
        "        target_file = \"english.txt\"\n",
        "        output_directory = \"output\"\n",
        "\n",
        "        print(f\"\\nProcessing files:\")\n",
        "        print(f\"Source: {source_file}\")\n",
        "        print(f\"Target: {target_file}\")\n",
        "        print(f\"Output: {output_directory}\")\n",
        "\n",
        "        aligned_file, excluded_file = process_files(\n",
        "            source_file,\n",
        "            target_file,\n",
        "            output_directory\n",
        "        )\n",
        "\n",
        "        print(\"\\nSuccess!\")\n",
        "        print(f\"Aligned sentences: {aligned_file}\")\n",
        "        print(f\"Excluded sentences: {excluded_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFailed to process files: {str(e)}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlUmsaiJlgqk",
        "outputId": "10c5a536-ba15-4f09-949f-1e8f440c9677"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing files:\n",
            "Source: spanish.txt\n",
            "Target: english.txt\n",
            "Output: output\n",
            "\n",
            "=== Starting File Processing ===\n",
            "Created output directory: output\n",
            "Detecting encoding for: spanish.txt\n",
            "Detected encoding: utf-8 (confidence: 0.99)\n",
            "Detecting encoding for: english.txt\n",
            "Detected encoding: ascii (confidence: 1.00)\n",
            "\n",
            "Reading input files...\n",
            "Read source file: 870 characters\n",
            "Read target file: 747 characters\n",
            "Validating file contents...\n",
            "File validation successful\n",
            "Validating file contents...\n",
            "File validation successful\n",
            "Initializing Sentence Aligner...\n",
            "Initializing Gale-Church Aligner...\n",
            "Initialization complete\n",
            "\n",
            "Starting sentence alignment process...\n",
            "\n",
            "Tokenizing sentences...\n",
            "Input text length: 870 characters\n",
            "Cleaned text length: 851 characters\n",
            "Found 11 sentences\n",
            "\n",
            "First few sentences found:\n",
            "1. El aprendizaje automático está transformando la forma en que interactuamos con la tecnología.\n",
            "2. Impulsa todo, desde sistemas de recomendación hasta vehículos autónomos. ##\n",
            "3. Conceptos Básicos ¡Las redes neuronales están inspiradas en el cerebro humano!\n",
            "\n",
            "Tokenizing sentences...\n",
            "Input text length: 747 characters\n",
            "Cleaned text length: 729 characters\n",
            "Found 11 sentences\n",
            "\n",
            "First few sentences found:\n",
            "1. Machine learning is transforming the way we interact with technology.\n",
            "2. It powers everything from recommendation systems to autonomous vehicles. ##\n",
            "3. Basic Concepts Neural networks are inspired by the human brain!\n",
            "Starting alignment of 11 source and 11 target sentences...\n",
            "Computing optimal alignments...\n",
            "Progress: 100/144 steps (69.4%)\n",
            "Reconstructing alignments...\n",
            "Found 6 alignments\n",
            "\n",
            "Processing aligned pairs...\n",
            "\n",
            "Alignment complete:\n",
            "- Aligned pairs: 6\n",
            "- Excluded pairs: 0\n",
            "\n",
            "Writing output files...\n",
            "Written 6 pairs to output/aligned.jsonl\n",
            "Written 0 pairs to output/excluded.jsonl\n",
            "\n",
            "=== File Processing Complete ===\n",
            "\n",
            "Success!\n",
            "Aligned sentences: output/aligned.jsonl\n",
            "Excluded sentences: output/excluded.jsonl\n"
          ]
        }
      ]
    }
  ]
}