{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJLBVQBr3oY3nU3k4Qp3A+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/01_chunking_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Umsij4JxwE6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class ChunkPair:\n",
        "    source: str\n",
        "    target: str\n",
        "    chunk_type: str  # 'sentence' or 'recursive'\n",
        "    alignment_score: float = 0.0\n",
        "\n",
        "class DocumentReader:\n",
        "    @staticmethod\n",
        "    def read_docx(file_path: str) -> str:\n",
        "        \"\"\"Read content from a .docx file.\"\"\"\n",
        "        doc = Document(file_path)\n",
        "        return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "    @staticmethod\n",
        "    def read_pdf(file_path: str) -> str:\n",
        "        \"\"\"Read content from a PDF file.\"\"\"\n",
        "        text = []\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                text.append(page.extract_text())\n",
        "        return '\\n'.join(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_document(file_path: str) -> str:\n",
        "        \"\"\"Read content from either .docx or .pdf file.\"\"\"\n",
        "        suffix = Path(file_path).suffix.lower()\n",
        "        if suffix == '.docx':\n",
        "            return DocumentReader.read_docx(file_path)\n",
        "        elif suffix == '.pdf':\n",
        "            return DocumentReader.read_pdf(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
        "\n",
        "class SentenceTokenizer:\n",
        "    def __init__(self, language: str):\n",
        "        \"\"\"Initialize tokenizer for a specific language.\"\"\"\n",
        "        self.language = language\n",
        "        # Create blank model\n",
        "        self.nlp = spacy.blank(language)\n",
        "        # Add sentencizer component\n",
        "        self.nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "        # Common abbreviations by language\n",
        "        self.abbreviations = {\n",
        "            'en': {'Mr.', 'Mrs.', 'Dr.', 'Ms.', 'Prof.', 'Sr.', 'Jr.', 'etc.'},\n",
        "            'es': {'Sr.', 'Sra.', 'Dr.', 'Dra.', 'Prof.', 'etc.'},\n",
        "            'fr': {'M.', 'Mme.', 'Dr.', 'Prof.', 'etc.'}\n",
        "        }\n",
        "\n",
        "    def protect_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Replace periods in abbreviations with a special marker.\"\"\"\n",
        "        protected_text = text\n",
        "        for abbr in self.abbreviations.get(self.language, set()):\n",
        "            protected_text = re.sub(\n",
        "                r'\\b' + re.escape(abbr) + r'\\b',\n",
        "                lambda m: m.group().replace('.', '@POINT@'),\n",
        "                protected_text\n",
        "            )\n",
        "        return protected_text\n",
        "\n",
        "    def restore_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Restore the original periods in abbreviations.\"\"\"\n",
        "        return text.replace('@POINT@', '.')\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        # Protect abbreviations\n",
        "        text = self.protect_abbreviations(text)\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = self.nlp(text)\n",
        "        sentences = [str(sent).strip() for sent in doc.sents]\n",
        "\n",
        "        # Restore abbreviations\n",
        "        sentences = [self.restore_abbreviations(sent) for sent in sentences]\n",
        "\n",
        "        return sentences\n",
        "\n",
        "class RecursiveChunker:\n",
        "    def __init__(self, min_chunk_size: int = 100, max_chunk_size: int = 500):\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "\n",
        "    def chunk_text(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Recursively chunk text while preserving sentence boundaries.\"\"\"\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            # If single sentence exceeds max_chunk_size, keep it as a separate chunk\n",
        "            if sentence_length > self.max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                    current_chunk = []\n",
        "                    current_length = 0\n",
        "                chunks.append(sentence)\n",
        "                continue\n",
        "\n",
        "            # If adding this sentence would exceed max_chunk_size\n",
        "            if current_length + sentence_length > self.max_chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Add remaining chunk if it exists\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class DocumentAligner:\n",
        "    def __init__(self, source_lang: str = 'es', target_lang: str = 'en'):\n",
        "        self.source_tokenizer = SentenceTokenizer(source_lang)\n",
        "        self.target_tokenizer = SentenceTokenizer(target_lang)\n",
        "        self.chunker = RecursiveChunker()\n",
        "\n",
        "    def calculate_length_ratio(self, source: str, target: str) -> float:\n",
        "        \"\"\"Calculate length ratio between source and target texts.\"\"\"\n",
        "        return len(target) / len(source) if len(source) > 0 else float('inf')\n",
        "\n",
        "    def align_chunks(self, source_chunks: List[str], target_chunks: List[str]) -> List[ChunkPair]:\n",
        "        \"\"\"Align chunks based on length ratios and position.\"\"\"\n",
        "        aligned_pairs = []\n",
        "        expected_ratio = sum(len(t) for t in target_chunks) / sum(len(s) for s in source_chunks)\n",
        "\n",
        "        for i, source_chunk in enumerate(source_chunks):\n",
        "            if i >= len(target_chunks):\n",
        "                print(f\"Warning: Missing target chunk for source chunk {i+1}\")\n",
        "                continue\n",
        "\n",
        "            target_chunk = target_chunks[i]\n",
        "            ratio = self.calculate_length_ratio(source_chunk, target_chunk)\n",
        "\n",
        "            # Check if ratio is within acceptable range (Â±30% of expected ratio)\n",
        "            if 0.7 * expected_ratio <= ratio <= 1.3 * expected_ratio:\n",
        "                aligned_pairs.append(ChunkPair(\n",
        "                    source=source_chunk,\n",
        "                    target=target_chunk,\n",
        "                    chunk_type='sentence' if len(source_chunk.split()) <= 2 else 'recursive',\n",
        "                    alignment_score=1.0 - abs(ratio - expected_ratio) / expected_ratio\n",
        "                ))\n",
        "            else:\n",
        "                print(f\"Warning: Possible misalignment in chunk {i+1}\")\n",
        "                print(f\"Source: {source_chunk[:100]}...\")\n",
        "                print(f\"Target: {target_chunk[:100]}...\")\n",
        "                aligned_pairs.append(ChunkPair(\n",
        "                    source=source_chunk,\n",
        "                    target=target_chunk,\n",
        "                    chunk_type='unverified',\n",
        "                    alignment_score=0.5\n",
        "                ))\n",
        "\n",
        "        return aligned_pairs\n",
        "\n",
        "    def process_documents(self, source_path: str, target_path: str) -> List[ChunkPair]:\n",
        "        \"\"\"Process and align documents.\"\"\"\n",
        "        # Read documents\n",
        "        source_text = DocumentReader.read_document(source_path)\n",
        "        target_text = DocumentReader.read_document(target_path)\n",
        "\n",
        "        # Get sentences\n",
        "        source_sentences = self.source_tokenizer.tokenize(source_text)\n",
        "        target_sentences = self.target_tokenizer.tokenize(target_text)\n",
        "\n",
        "        # Create chunks\n",
        "        source_chunks = self.chunker.chunk_text(source_sentences)\n",
        "        target_chunks = self.chunker.chunk_text(target_sentences)\n",
        "\n",
        "        # Align chunks\n",
        "        return self.align_chunks(source_chunks, target_chunks)\n",
        "\n",
        "def process_alignment_file(input_path: str, output_path: str):\n",
        "    \"\"\"Process a JSONL file containing document pairs and create aligned chunks.\"\"\"\n",
        "    aligner = DocumentAligner()\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = Path(output_path).parent\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    aligned_pairs = []\n",
        "\n",
        "    # Read input file\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            pair = json.loads(line)\n",
        "            source_path = pair['source']\n",
        "            target_path = pair['target']\n",
        "\n",
        "            print(f\"\\nProcessing document pair:\")\n",
        "            print(f\"Source: {source_path}\")\n",
        "            print(f\"Target: {target_path}\")\n",
        "\n",
        "            # Process document pair\n",
        "            chunk_pairs = aligner.process_documents(source_path, target_path)\n",
        "\n",
        "            # Convert to required output format\n",
        "            for pair in chunk_pairs:\n",
        "                aligned_pairs.append({\n",
        "                    \"source_text\": pair.source,\n",
        "                    \"references\": [pair.target],\n",
        "                    \"chunk_type\": pair.chunk_type,\n",
        "                    \"alignment_score\": pair.alignment_score\n",
        "                })\n",
        "\n",
        "    # Write output file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for pair in aligned_pairs:\n",
        "            json.dump(pair, f, ensure_ascii=False)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Total aligned pairs: {len(aligned_pairs)}\")\n",
        "    print(f\"Output written to: {output_path}\")\n",
        "\n",
        "# Example usage in Jupyter notebook\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file paths\n",
        "    input_file = \"document_pairs.jsonl\"\n",
        "    output_file = \"aligned_chunks.jsonl\"\n",
        "\n",
        "    process_alignment_file(input_file, output_file)"
      ]
    }
  ]
}