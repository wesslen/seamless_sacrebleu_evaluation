{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf/uDGt7VjPyslhg5tbPfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/01_chunking_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import PyPDF2\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class ChunkPair:\n",
        "    source: str\n",
        "    target: str\n",
        "    chunk_type: str  # 'sentence' or 'recursive'\n",
        "    alignment_score: float = 0.0\n",
        "\n",
        "class DocumentReader:\n",
        "    @staticmethod\n",
        "    def read_text(file_path: str) -> str:\n",
        "        \"\"\"Read content from a text file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    @staticmethod\n",
        "    def read_pdf(file_path: str) -> str:\n",
        "        \"\"\"Read content from a PDF file.\"\"\"\n",
        "        text = []\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                text.append(page.extract_text())\n",
        "        return '\\n'.join(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_document(file_path: str) -> str:\n",
        "        \"\"\"Read content from either text or pdf file.\"\"\"\n",
        "        suffix = Path(file_path).suffix.lower()\n",
        "        if suffix in ['.txt', '.md']:\n",
        "            return DocumentReader.read_text(file_path)\n",
        "        elif suffix == '.pdf':\n",
        "            return DocumentReader.read_pdf(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
        "\n",
        "class SentenceTokenizer:\n",
        "    def __init__(self, language: str):\n",
        "        \"\"\"Initialize tokenizer for a specific language.\"\"\"\n",
        "        self.language = language\n",
        "        # Create blank model\n",
        "        self.nlp = spacy.blank(language)\n",
        "        # Add sentencizer component\n",
        "        self.nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "        # Common abbreviations by language\n",
        "        self.abbreviations = {\n",
        "            'en': {'Mr.', 'Mrs.', 'Dr.', 'Ms.', 'Prof.', 'Sr.', 'Jr.', 'etc.'},\n",
        "            'es': {'Sr.', 'Sra.', 'Dr.', 'Dra.', 'Prof.', 'etc.'},\n",
        "            'fr': {'M.', 'Mme.', 'Dr.', 'Prof.', 'etc.'}\n",
        "        }\n",
        "\n",
        "        # Sentence ending punctuation by language\n",
        "        self.sent_endings = {\n",
        "            'en': {'!', '?', '.', ';\"'},\n",
        "            'es': {'!', '¡', '?', '¿', '.', ';'},\n",
        "            'fr': {'!', '?', '.', ';'}\n",
        "        }\n",
        "\n",
        "    def protect_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Replace periods in abbreviations with a special marker.\"\"\"\n",
        "        protected_text = text\n",
        "        for abbr in self.abbreviations.get(self.language, set()):\n",
        "            protected_text = re.sub(\n",
        "                r'\\b' + re.escape(abbr) + r'\\b',\n",
        "                lambda m: m.group().replace('.', '@POINT@'),\n",
        "                protected_text\n",
        "            )\n",
        "        return protected_text\n",
        "\n",
        "    def restore_abbreviations(self, text: str) -> str:\n",
        "        \"\"\"Restore the original periods in abbreviations.\"\"\"\n",
        "        return text.replace('@POINT@', '.')\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean text while preserving important whitespace and structure.\"\"\"\n",
        "        # Replace multiple newlines with single newline\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "        # Replace multiple spaces with single space\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # Clean up whitespace around punctuation\n",
        "        text = re.sub(r'\\s+([.,!?;])', r'\\1', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        # Clean and protect text\n",
        "        text = self.clean_text(text)\n",
        "        text = self.protect_abbreviations(text)\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = self.nlp(text)\n",
        "        sentences = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            sent_text = str(sent).strip()\n",
        "            # Only add if it ends with sentence-ending punctuation\n",
        "            if sent_text and sent_text[-1] in self.sent_endings.get(self.language, {'.', '!', '?'}):\n",
        "                sentences.append(self.restore_abbreviations(sent_text))\n",
        "\n",
        "        return sentences\n",
        "\n",
        "class RecursiveChunker:\n",
        "    def __init__(self, min_chunk_size: int = 100, max_chunk_size: int = 500):\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "\n",
        "    def chunk_text(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Recursively chunk text while preserving sentence boundaries.\"\"\"\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            # If single sentence exceeds max_chunk_size, keep it as a separate chunk\n",
        "            if sentence_length > self.max_chunk_size:\n",
        "                if current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                    current_chunk = []\n",
        "                    current_length = 0\n",
        "                chunks.append(sentence)\n",
        "                continue\n",
        "\n",
        "            # If adding this sentence would exceed max_chunk_size\n",
        "            if current_length + sentence_length > self.max_chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Add remaining chunk if it exists\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class TextStatistics:\n",
        "    @staticmethod\n",
        "    def calculate_similarity_score(source: str, target: str) -> float:\n",
        "        \"\"\"Calculate a similarity score based on various text statistics.\"\"\"\n",
        "        # Length ratio\n",
        "        len_ratio = len(target) / len(source) if len(source) > 0 else float('inf')\n",
        "\n",
        "        # Sentence count ratio\n",
        "        source_sent_count = len(re.findall(r'[.!?]+', source))\n",
        "        target_sent_count = len(re.findall(r'[.!?]+', target))\n",
        "        sent_ratio = target_sent_count / source_sent_count if source_sent_count > 0 else float('inf')\n",
        "\n",
        "        # Normalize ratios to 0-1 range\n",
        "        len_score = max(0, 1 - abs(1 - len_ratio))\n",
        "        sent_score = max(0, 1 - abs(1 - sent_ratio))\n",
        "\n",
        "        # Combine scores\n",
        "        return (len_score + sent_score) / 2\n",
        "\n",
        "class DocumentAligner:\n",
        "    def __init__(self, source_lang: str = 'es', target_lang: str = 'en'):\n",
        "        self.source_tokenizer = SentenceTokenizer(source_lang)\n",
        "        self.target_tokenizer = SentenceTokenizer(target_lang)\n",
        "        self.chunker = RecursiveChunker()\n",
        "\n",
        "    def verify_alignment(self, source: str, target: str) -> Tuple[bool, float]:\n",
        "        \"\"\"Verify if two chunks are likely to be aligned.\"\"\"\n",
        "        similarity_score = TextStatistics.calculate_similarity_score(source, target)\n",
        "        is_aligned = similarity_score >= 0.7  # threshold for alignment\n",
        "        return is_aligned, similarity_score\n",
        "\n",
        "    def align_chunks(self, source_chunks: List[str], target_chunks: List[str]) -> List[ChunkPair]:\n",
        "        \"\"\"Align chunks based on similarity scores and position.\"\"\"\n",
        "        aligned_pairs = []\n",
        "\n",
        "        if len(source_chunks) != len(target_chunks):\n",
        "            print(f\"Warning: Number of chunks differs - Source: {len(source_chunks)}, Target: {len(target_chunks)}\")\n",
        "\n",
        "        for i, source_chunk in enumerate(source_chunks):\n",
        "            if i >= len(target_chunks):\n",
        "                print(f\"Warning: Missing target chunk for source chunk {i+1}\")\n",
        "                continue\n",
        "\n",
        "            target_chunk = target_chunks[i]\n",
        "            is_aligned, similarity_score = self.verify_alignment(source_chunk, target_chunk)\n",
        "\n",
        "            chunk_type = 'sentence' if len(source_chunk.split()) <= 2 else 'recursive'\n",
        "\n",
        "            if not is_aligned:\n",
        "                print(f\"\\nWarning: Possible misalignment in chunk {i+1}\")\n",
        "                print(f\"Source: {source_chunk[:100]}...\")\n",
        "                print(f\"Target: {target_chunk[:100]}...\")\n",
        "                chunk_type = 'unverified'\n",
        "\n",
        "            aligned_pairs.append(ChunkPair(\n",
        "                source=source_chunk,\n",
        "                target=target_chunk,\n",
        "                chunk_type=chunk_type,\n",
        "                alignment_score=similarity_score\n",
        "            ))\n",
        "\n",
        "        return aligned_pairs\n",
        "\n",
        "    def process_documents(self, source_path: str, target_path: str) -> List[ChunkPair]:\n",
        "        \"\"\"Process and align documents.\"\"\"\n",
        "        print(f\"\\nProcessing documents:\")\n",
        "        print(f\"Source: {source_path}\")\n",
        "        print(f\"Target: {target_path}\")\n",
        "\n",
        "        # Read documents\n",
        "        source_text = DocumentReader.read_document(source_path)\n",
        "        target_text = DocumentReader.read_document(target_path)\n",
        "\n",
        "        print(\"Tokenizing sentences...\")\n",
        "        # Get sentences\n",
        "        source_sentences = self.source_tokenizer.tokenize(source_text)\n",
        "        target_sentences = self.target_tokenizer.tokenize(target_text)\n",
        "\n",
        "        print(f\"Found {len(source_sentences)} source sentences and {len(target_sentences)} target sentences\")\n",
        "\n",
        "        print(\"Creating chunks...\")\n",
        "        # Create chunks\n",
        "        source_chunks = self.chunker.chunk_text(source_sentences)\n",
        "        target_chunks = self.chunker.chunk_text(target_sentences)\n",
        "\n",
        "        print(f\"Created {len(source_chunks)} source chunks and {len(target_chunks)} target chunks\")\n",
        "\n",
        "        print(\"Aligning chunks...\")\n",
        "        # Align chunks\n",
        "        return self.align_chunks(source_chunks, target_chunks)\n",
        "\n",
        "def process_alignment_file(input_path: str, output_path: str):\n",
        "    \"\"\"Process a JSONL file containing document pairs and create aligned chunks.\"\"\"\n",
        "    aligner = DocumentAligner()\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = Path(output_path).parent\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    aligned_pairs = []\n",
        "\n",
        "    # Read input file\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            pair = json.loads(line)\n",
        "            source_path = pair['source']\n",
        "            target_path = pair['target']\n",
        "\n",
        "            # Process document pair\n",
        "            chunk_pairs = aligner.process_documents(source_path, target_path)\n",
        "\n",
        "            # Convert to required output format\n",
        "            for pair in chunk_pairs:\n",
        "                if pair.alignment_score >= 0.7:  # Only include high-confidence alignments\n",
        "                    aligned_pairs.append({\n",
        "                        \"source_text\": pair.source,\n",
        "                        \"references\": [pair.target],\n",
        "                        \"chunk_type\": pair.chunk_type,\n",
        "                        \"alignment_score\": pair.alignment_score\n",
        "                    })\n",
        "\n",
        "    # Write output file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for pair in aligned_pairs:\n",
        "            json.dump(pair, f, ensure_ascii=False)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Total aligned pairs: {len(aligned_pairs)}\")\n",
        "    print(f\"Output written to: {output_path}\")\n",
        "\n",
        "# Example usage in Jupyter notebook\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file paths\n",
        "    input_file = \"document_pairs.jsonl\"\n",
        "    output_file = \"aligned_chunks.jsonl\"\n",
        "\n",
        "    process_alignment_file(input_file, output_file)"
      ],
      "metadata": {
        "id": "f64GPBfJ7IA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}