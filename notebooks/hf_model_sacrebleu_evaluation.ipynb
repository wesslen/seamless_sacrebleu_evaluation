{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN3KAx6DL70wIS3fyAl67Tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "698152e049594959a64975e4cba0a873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a76781d9882a48a9b88dc80d5bf91db4",
              "IPY_MODEL_6ef422ee61604ceabd56c5e3cc08488a",
              "IPY_MODEL_d99ea488c9a64d749119a1caa48af1ae"
            ],
            "layout": "IPY_MODEL_77e4dc15b72d4ad7b3124cd3bca6ca05"
          }
        },
        "a76781d9882a48a9b88dc80d5bf91db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_136cccc16f7f452b810c9673b6ca5899",
            "placeholder": "​",
            "style": "IPY_MODEL_7ebf4e397b7a4370b8bd5935289a5ea3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6ef422ee61604ceabd56c5e3cc08488a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7aac2a0bf154ffcaa8d24a51331275a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79b730fdd9cd4c3abb9e99c90103236b",
            "value": 4
          }
        },
        "d99ea488c9a64d749119a1caa48af1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0297f7d99ff444ca09b99bebf612185",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c5333414b94a809572891dacb3efb2",
            "value": " 4/4 [00:09&lt;00:00,  2.10s/it]"
          }
        },
        "77e4dc15b72d4ad7b3124cd3bca6ca05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136cccc16f7f452b810c9673b6ca5899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ebf4e397b7a4370b8bd5935289a5ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7aac2a0bf154ffcaa8d24a51331275a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79b730fdd9cd4c3abb9e99c90103236b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0297f7d99ff444ca09b99bebf612185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c5333414b94a809572891dacb3efb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2c9b5b302d45deb7ead45c7a76d309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c299468f12154abb9bb0f3ab7e6fba28",
              "IPY_MODEL_d91c574d55d74bb5a3a74f60ed4d1668",
              "IPY_MODEL_ca275f2845d141d3b41bebeadededed6"
            ],
            "layout": "IPY_MODEL_e3f82476b5834434962f4d10446f1faf"
          }
        },
        "c299468f12154abb9bb0f3ab7e6fba28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61fccdc883ad4dc6a9411b12674998ea",
            "placeholder": "​",
            "style": "IPY_MODEL_7db77cd078074ee7ba70b1a39a04cdc4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d91c574d55d74bb5a3a74f60ed4d1668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69bc977fd5cc44a2b70951a4687ef385",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76bc8aa7a1c24ed6b6a20e9da38c2162",
            "value": 2
          }
        },
        "ca275f2845d141d3b41bebeadededed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2222519453c243ea8bff451153ceb03b",
            "placeholder": "​",
            "style": "IPY_MODEL_e0bf386b322c4a02b619f1faa96b5211",
            "value": " 2/2 [00:04&lt;00:00,  1.83s/it]"
          }
        },
        "e3f82476b5834434962f4d10446f1faf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61fccdc883ad4dc6a9411b12674998ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db77cd078074ee7ba70b1a39a04cdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69bc977fd5cc44a2b70951a4687ef385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bc8aa7a1c24ed6b6a20e9da38c2162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2222519453c243ea8bff451153ceb03b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0bf386b322c4a02b619f1faa96b5211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/hf_model_sacrebleu_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv8rGGsgBYT4",
        "outputId": "de7437b3-82aa-4a1c-cc50-28f5d8133369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.10.12 environment at /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system transformers sacrebleu tqdm torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Any, List, Union\n",
        "from enum import Enum\n",
        "import json\n",
        "import logging\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InferenceMode(Enum):\n",
        "    \"\"\"Enum for inference modes\"\"\"\n",
        "    LOCAL = \"local\"\n",
        "    API = \"api\"\n",
        "\n",
        "@dataclass\n",
        "class APIConfig:\n",
        "    \"\"\"Configuration for API inference\"\"\"\n",
        "    base_url: str\n",
        "    api_key: str\n",
        "    model: str\n",
        "    timeout: int = 30\n",
        "    max_retries: int = 3\n",
        "\n",
        "    def validate(self) -> None:\n",
        "        \"\"\"Validate API configuration\"\"\"\n",
        "        if not self.base_url:\n",
        "            raise ValueError(\"base_url is required for API inference\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"api_key is required for API inference\")\n",
        "        if not self.model:\n",
        "            raise ValueError(\"model is required for API inference\")\n",
        "\n",
        "@dataclass\n",
        "class Llama31Config:\n",
        "    \"\"\"Configuration for Llama 3.1 specific settings\"\"\"\n",
        "    SUPPORTED_LANGUAGES = {\n",
        "        \"English\": \"eng\", \"Spanish\": \"spa\", \"French\": \"fra\",\n",
        "        \"German\": \"deu\", \"Italian\": \"ita\", \"Portuguese\": \"por\",\n",
        "        \"Hindi\": \"hin\", \"Thai\": \"tha\"\n",
        "    }\n",
        "\n",
        "    system_prompt: str = \"\"\"You are a professional translator with expertise in multiple languages.\n",
        "    Your task is to translate the provided text accurately while preserving meaning, tone, and context.\n",
        "    Only provide the direct translation without any explanations or notes.\"\"\"\n",
        "\n",
        "    # Modified template to be more explicit\n",
        "    chat_template: str = (\n",
        "        \"<|begin_of_text|>\"\n",
        "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
        "        \"{system_prompt}\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
        "        \"{user_prompt}\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "    )\n",
        "\n",
        "    def format_prompt(self, text: str, src_lang: str, tgt_lang: str, for_api: bool = False) -> Union[str, List[Dict[str, str]]]:\n",
        "        \"\"\"Format the translation prompt\"\"\"\n",
        "        user_prompt = f\"Translate the following text from {src_lang} to {tgt_lang}. Provide only the translation:\\n\\n{text}\"\n",
        "\n",
        "        if for_api:\n",
        "            return [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        else:\n",
        "            return self.chat_template.format(\n",
        "                system_prompt=self.system_prompt,\n",
        "                user_prompt=user_prompt\n",
        "            )\n",
        "\n",
        "@dataclass\n",
        "class TranslationConfig:\n",
        "    \"\"\"Configuration for translation settings\"\"\"\n",
        "    src_lang: str\n",
        "    tgt_lang: str\n",
        "    inference_mode: InferenceMode = InferenceMode.LOCAL\n",
        "    api_config: Optional[APIConfig] = None\n",
        "\n",
        "    # Generation settings that work for both local and API\n",
        "    max_new_tokens: int = 256\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.95\n",
        "    top_k: int = 50\n",
        "    repetition_penalty: float = 1.2\n",
        "    num_beams: int = 4\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization\"\"\"\n",
        "        if self.inference_mode == InferenceMode.API and not self.api_config:\n",
        "            raise ValueError(\"api_config is required when inference_mode is API\")\n",
        "\n",
        "        if self.api_config:\n",
        "            self.api_config.validate()\n",
        "\n",
        "    def get_api_parameters(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get parameters formatted for API call\"\"\"\n",
        "        return {\n",
        "            \"max_tokens\": self.max_new_tokens,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"presence_penalty\": self.repetition_penalty - 1.0,  # Convert to OpenAI format\n",
        "        }\n",
        "\n",
        "    def get_local_parameters(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get parameters formatted for local inference\"\"\"\n",
        "        return {\n",
        "            \"max_new_tokens\": self.max_new_tokens,\n",
        "            \"num_beams\": self.num_beams,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"repetition_penalty\": self.repetition_penalty,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_preset(\n",
        "        cls,\n",
        "        preset: str,\n",
        "        src_lang: str,\n",
        "        tgt_lang: str,\n",
        "        inference_mode: InferenceMode = InferenceMode.LOCAL,\n",
        "        api_config: Optional[APIConfig] = None\n",
        "    ) -> 'TranslationConfig':\n",
        "        \"\"\"Create config from preset generation strategy\"\"\"\n",
        "        base_config = cls(\n",
        "            src_lang=src_lang,\n",
        "            tgt_lang=tgt_lang,\n",
        "            inference_mode=inference_mode,\n",
        "            api_config=api_config\n",
        "        )\n",
        "\n",
        "        if preset == \"beam_search\":\n",
        "            base_config.num_beams = 5\n",
        "            base_config.temperature = 1.0\n",
        "        elif preset == \"sampling\":\n",
        "            base_config.num_beams = 1\n",
        "            base_config.temperature = 0.7\n",
        "            base_config.top_p = 0.9\n",
        "        elif preset == \"quality\":\n",
        "            base_config.num_beams = 4\n",
        "            base_config.temperature = 0.7\n",
        "            base_config.top_p = 0.95\n",
        "            base_config.repetition_penalty = 1.2\n",
        "\n",
        "        return base_config"
      ],
      "metadata": {
        "id": "E3VMEglxBhWj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APIClient:\n",
        "    \"\"\"Client for making API calls with retry logic\"\"\"\n",
        "\n",
        "    def __init__(self, api_config: APIConfig):\n",
        "        \"\"\"Initialize API client with retry logic\"\"\"\n",
        "        self.api_config = api_config\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self) -> requests.Session:\n",
        "        \"\"\"Create session with retry logic\"\"\"\n",
        "        session = requests.Session()\n",
        "\n",
        "        # Configure retry strategy\n",
        "        retry_strategy = Retry(\n",
        "            total=self.api_config.max_retries,\n",
        "            backoff_factor=0.5,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "        )\n",
        "\n",
        "        # Add retry adapter to session\n",
        "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "\n",
        "        return session\n",
        "\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(3),\n",
        "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "        reraise=True\n",
        "    )\n",
        "    def generate_translation(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        parameters: Dict[str, Any]\n",
        "    ) -> str:\n",
        "        \"\"\"Make API call with retry logic\"\"\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {self.api_config.api_key}\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.api_config.model,\n",
        "            \"messages\": messages,\n",
        "            **parameters\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                f\"{self.api_config.base_url}chat/completions\",\n",
        "                headers=headers,\n",
        "                json=data,\n",
        "                timeout=self.api_config.timeout\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"API call failed: {str(e)}\")\n",
        "            raise RuntimeError(f\"Failed to get translation from API: {str(e)}\")"
      ],
      "metadata": {
        "id": "yGx67JtZBqto"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sacrebleu import BLEU\n",
        "import tqdm\n",
        "\n",
        "class Llama31TranslationEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: TranslationConfig,\n",
        "        model_name: Optional[str] = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        debug: bool = False\n",
        "    ):\n",
        "        \"\"\"Initialize the translation evaluator\"\"\"\n",
        "        self.debug = debug\n",
        "        self.config = config\n",
        "        self.llama_config = Llama31Config()\n",
        "\n",
        "        if config.inference_mode == InferenceMode.API:\n",
        "            logger.info(\"Initializing API client...\")\n",
        "            self.api_client = APIClient(config.api_config)\n",
        "            self.inference_fn = self._translate_api\n",
        "        else:\n",
        "            logger.info(f\"Loading local model on device: {device}\")\n",
        "            self.device = device\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            # Handle tokenizer settings\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.padding_side = \"left\"\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            self.inference_fn = self._translate_local\n",
        "\n",
        "        self.bleu = BLEU()\n",
        "        logger.info(\"Setup complete!\")\n",
        "\n",
        "    def _translate_api(self, text: str, batch_idx: int = 0) -> str:\n",
        "        \"\"\"Translate using API endpoint\"\"\"\n",
        "        try:\n",
        "            messages = self.llama_config.format_prompt(\n",
        "                text=text,\n",
        "                src_lang=self.config.src_lang,\n",
        "                tgt_lang=self.config.tgt_lang,\n",
        "                for_api=True  # Get API format\n",
        "            )\n",
        "\n",
        "            parameters = self.config.get_api_parameters()\n",
        "            translation = self.api_client.generate_translation(messages, parameters)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"API translation for batch {batch_idx}: {translation}\")\n",
        "\n",
        "            return translation.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"API translation failed for batch {batch_idx}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _translate_local(self, text: str, batch_idx: int = 0) -> str:\n",
        "        \"\"\"Translate using local model with detailed debugging\"\"\"\n",
        "        try:\n",
        "            # Generate the prompt\n",
        "            prompt = self.llama_config.format_prompt(\n",
        "                text=text,\n",
        "                src_lang=self.config.src_lang,\n",
        "                tgt_lang=self.config.tgt_lang,\n",
        "                for_api=False\n",
        "            )\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"\\nProcessing batch {batch_idx}\")\n",
        "                logger.info(f\"Input text: {text}\")\n",
        "                logger.info(f\"Generated prompt:\\n{prompt}\")\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            ).to(self.device)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Input token count: {len(inputs['input_ids'][0])}\")\n",
        "\n",
        "            # Setup generation parameters\n",
        "            parameters = self.config.get_local_parameters()\n",
        "            parameters.update({\n",
        "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "                \"do_sample\": parameters[\"temperature\"] > 0,\n",
        "            })\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Generation parameters: {parameters}\")\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(**inputs, **parameters)\n",
        "\n",
        "            # Decode full output\n",
        "            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Full model output:\\n{full_output}\")\n",
        "\n",
        "            # Extract translation - look for assistant's response\n",
        "            assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "            eot_marker = \"<|eot_id|>\"\n",
        "\n",
        "            if assistant_marker in full_output:\n",
        "                response_start = full_output.find(assistant_marker) + len(assistant_marker)\n",
        "                response_end = full_output.find(eot_marker, response_start)\n",
        "\n",
        "                if response_end != -1:\n",
        "                    translation = full_output[response_start:response_end]\n",
        "                else:\n",
        "                    translation = full_output[response_start:]\n",
        "            else:\n",
        "                # Fallback: try to extract everything after the prompt\n",
        "                translation = full_output[len(prompt):]\n",
        "\n",
        "            # Clean up the translation\n",
        "            translation = translation.replace(assistant_marker, \"\").replace(eot_marker, \"\")\n",
        "            translation = translation.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Extracted translation: {translation}\")\n",
        "                if not translation:\n",
        "                    logger.warning(\"Warning: Empty translation detected!\")\n",
        "\n",
        "            return translation\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Local translation failed for batch {batch_idx}: {str(e)}\")\n",
        "            logger.error(f\"Error details:\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def translate_batch(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        batch_size: int = 1\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Translate a list of texts\"\"\"\n",
        "        translations = []\n",
        "\n",
        "        for i in tqdm.trange(0, len(texts), batch_size, desc=\"Translating\"):\n",
        "            batch = texts[i:i + batch_size]\n",
        "\n",
        "            batch_translations = []\n",
        "            for j, text in enumerate(batch):\n",
        "                try:\n",
        "                    translation = self.inference_fn(text, i + j)\n",
        "                    batch_translations.append(translation)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Translation failed for text {i + j}: {str(e)}\")\n",
        "                    batch_translations.append(\"\")\n",
        "\n",
        "            translations.extend(batch_translations)\n",
        "\n",
        "        return translations\n",
        "\n",
        "    def evaluate_translations(\n",
        "        self,\n",
        "        hypotheses: List[str],\n",
        "        references: Union[List[str], List[List[str]]],\n",
        "        verbose: bool = True\n",
        "    ) -> BLEU:\n",
        "        \"\"\"Evaluate translations using sacreBLEU\"\"\"\n",
        "        if isinstance(references[0], str):\n",
        "            references = [[ref] for ref in references]\n",
        "\n",
        "        bleu_score = self.bleu.corpus_score(hypotheses, references)\n",
        "\n",
        "        if verbose:\n",
        "            logger.info(f\"BLEU score: {bleu_score.score:.2f}\")\n",
        "            logger.info(f\"Signature: {self.bleu.get_signature()}\")\n",
        "\n",
        "        return bleu_score"
      ],
      "metadata": {
        "id": "ItGE5uxYBsxB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "BASE_URL = userdata.get('MODAL_BASE_URL') # should end in /v1/\n",
        "API_KEY = userdata.get('DSBA_LLAMA3_KEY')\n",
        "model_name = \"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "5SCReLkACCAv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        "### API"
      ],
      "metadata": {
        "id": "spZ3OkU9Jobt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test data\n",
        "source_texts = [\n",
        "    \"Hello, my dog is cute\",\n",
        "    \"The weather is nice today\",\n",
        "    \"I love programming\"\n",
        "]\n",
        "\n",
        "# Reference translations in French\n",
        "references = [\n",
        "    [\"Bonjour, mon chien est mignon\"],\n",
        "    [\"Le temps est beau aujourd'hui\"],\n",
        "    [\"J'aime la programmation\"]\n",
        "]\n",
        "\n",
        "# API Configuration\n",
        "api_config = APIConfig(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "# Translation Configuration\n",
        "config = TranslationConfig.from_preset(\n",
        "    preset=\"sampling\",\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"French\",\n",
        "    inference_mode=InferenceMode.API,\n",
        "    api_config=api_config\n",
        ")\n",
        "\n",
        "print(\"Initializing translator...\")\n",
        "# Initialize evaluator with API mode\n",
        "evaluator = Llama31TranslationEvaluator(\n",
        "    config=config,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\nTranslating texts...\")\n",
        "# Translate\n",
        "translations = evaluator.translate_batch(\n",
        "    texts=source_texts,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for src, trans, ref in zip(source_texts, translations, references):\n",
        "    print(f\"\\nSource: {src}\")\n",
        "    print(f\"\\nTranslation: {trans}\")\n",
        "    print(f\"Reference: {ref[0]}\")\n",
        "\n",
        "# Evaluate translations\n",
        "bleu_score = evaluator.evaluate_translations(\n",
        "    hypotheses=translations,\n",
        "    references=references,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg0LDXF7B-WB",
        "outputId": "b0ccf828-1b8e-49ad-fb17-c9409a99f456"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing translator...\n",
            "\n",
            "Translating texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "\n",
            "Source: Hello, my dog is cute\n",
            "\n",
            "Translation: Bonjour, mon chien est mignon.\n",
            "Reference: Bonjour, mon chien est mignon\n",
            "\n",
            "Source: The weather is nice today\n",
            "\n",
            "Translation: Le temps est agréable aujourd'hui.\n",
            "Reference: Le temps est beau aujourd'hui\n",
            "\n",
            "Source: I love programming\n",
            "\n",
            "Translation: J'adore le développement informatique.\n",
            "Reference: J'aime la programmation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pPcvRRYDWqI",
        "outputId": "07183175-9f2c-4d34-fa46-0eaa03571e33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 80.91 85.7/83.3/80.0/75.0 (BP = 1.000 ratio = 1.167 hyp_len = 7 ref_len = 6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local"
      ],
      "metadata": {
        "id": "iv1_Hd4bJrZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with debug mode\n",
        "config = TranslationConfig.from_preset(\n",
        "    preset=\"quality\",\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"French\",\n",
        "    inference_mode=InferenceMode.LOCAL\n",
        ")\n",
        "\n",
        "print(\"Initializing translator...\")\n",
        "# Initialize evaluator with API mode\n",
        "evaluator = Llama31TranslationEvaluator(\n",
        "    config=config,\n",
        "    model_name=\"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\nTranslating texts...\")\n",
        "# Translate\n",
        "translations = evaluator.translate_batch(\n",
        "    texts=source_texts,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for src, trans, ref in zip(source_texts, translations, references):\n",
        "    print(f\"\\nSource: {src}\")\n",
        "    print(f\"\\nTranslation: {trans}\")\n",
        "    print(f\"Reference: {ref[0]}\")\n",
        "\n",
        "# Evaluate translations\n",
        "bleu_score = evaluator.evaluate_translations(\n",
        "    hypotheses=translations,\n",
        "    references=references,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "698152e049594959a64975e4cba0a873",
            "a76781d9882a48a9b88dc80d5bf91db4",
            "6ef422ee61604ceabd56c5e3cc08488a",
            "d99ea488c9a64d749119a1caa48af1ae",
            "77e4dc15b72d4ad7b3124cd3bca6ca05",
            "136cccc16f7f452b810c9673b6ca5899",
            "7ebf4e397b7a4370b8bd5935289a5ea3",
            "c7aac2a0bf154ffcaa8d24a51331275a",
            "79b730fdd9cd4c3abb9e99c90103236b",
            "e0297f7d99ff444ca09b99bebf612185",
            "d0c5333414b94a809572891dacb3efb2"
          ]
        },
        "id": "01FIV4JkJsmr",
        "outputId": "6b804df0-0f46-4389-8fb3-dd37079d7336"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing translator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "698152e049594959a64975e4cba0a873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translating texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "\n",
            "Source: Hello, my dog is cute\n",
            "\n",
            "Translation: Bonjour, mon chien est mignon.\n",
            "Reference: Bonjour, mon chien est mignon\n",
            "\n",
            "Source: The weather is nice today\n",
            "\n",
            "Translation: Le temps est agréable aujourd'hui.\n",
            "Reference: Le temps est beau aujourd'hui\n",
            "\n",
            "Source: I love programming\n",
            "\n",
            "Translation: J'adore programmer\n",
            "Reference: J'aime la programmation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgl8GvmyK-To",
        "outputId": "41824cfc-6c75-4443-9602-f5eb74e38914"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 80.91 85.7/83.3/80.0/75.0 (BP = 1.000 ratio = 1.167 hyp_len = 7 ref_len = 6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "laySEAoHEIN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/01-english-spanish-mapping.jsonl\n",
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/02-english-spanish-mapping.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kShoIGj4ELDO",
        "outputId": "46695e45-7137-40d7-ddb8-8b44dde2acf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 23:04:28--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/01-english-spanish-mapping.jsonl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/01-english-spanish-mapping.jsonl [following]\n",
            "--2024-12-01 23:04:28--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/01-english-spanish-mapping.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2829 (2.8K) [text/plain]\n",
            "Saving to: ‘01-english-spanish-mapping.jsonl.2’\n",
            "\n",
            "01-english-spanish- 100%[===================>]   2.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 23:04:28 (53.5 MB/s) - ‘01-english-spanish-mapping.jsonl.2’ saved [2829/2829]\n",
            "\n",
            "--2024-12-01 23:04:28--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/02-english-spanish-mapping.jsonl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/02-english-spanish-mapping.jsonl [following]\n",
            "--2024-12-01 23:04:28--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/02-english-spanish-mapping.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12284 (12K) [text/plain]\n",
            "Saving to: ‘02-english-spanish-mapping.jsonl.2’\n",
            "\n",
            "02-english-spanish- 100%[===================>]  12.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 23:04:29 (128 MB/s) - ‘02-english-spanish-mapping.jsonl.2’ saved [12284/12284]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def run_evaluation(\n",
        "    jsonl_files: List[str],\n",
        "    src_lang: str = \"English\",\n",
        "    tgt_lang: str = \"Spanish\",\n",
        "    inference_mode: InferenceMode = InferenceMode.LOCAL,\n",
        "    model_name: Optional[str] = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    api_config: Optional[APIConfig] = None,\n",
        "    batch_size: int = 4,\n",
        "    generation_preset: str = \"quality\",\n",
        "    debug: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Run evaluation pipeline with support for both local and API inference.\n",
        "\n",
        "    Args:\n",
        "        jsonl_files: List of JSONL files containing test data\n",
        "        src_lang: Source language\n",
        "        tgt_lang: Target language\n",
        "        inference_mode: Whether to use local model or API endpoint\n",
        "        model_name: HuggingFace model identifier (for local inference)\n",
        "        api_config: API configuration (for API inference)\n",
        "        batch_size: Batch size for processing\n",
        "        generation_preset: Generation strategy preset\n",
        "        debug: Enable debug logging\n",
        "    \"\"\"\n",
        "    logger.info(f\"\\nInitializing translator in {inference_mode.value} mode...\")\n",
        "\n",
        "    # Create translation config\n",
        "    config = TranslationConfig.from_preset(\n",
        "        preset=generation_preset,\n",
        "        src_lang=src_lang,\n",
        "        tgt_lang=tgt_lang,\n",
        "        inference_mode=inference_mode,\n",
        "        api_config=api_config\n",
        "    )\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = Llama31TranslationEvaluator(\n",
        "        config=config,\n",
        "        model_name=model_name if inference_mode == InferenceMode.LOCAL else None,\n",
        "        debug=debug\n",
        "    )\n",
        "\n",
        "    all_bleu_scores = []\n",
        "    results = {}\n",
        "\n",
        "    for file_path in jsonl_files:\n",
        "        logger.info(f\"\\nProcessing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Load data\n",
        "            mapping = read_jsonl(file_path)\n",
        "            source_texts = [item[\"source_text\"] for item in mapping]\n",
        "            references = [item[\"references\"] for item in mapping]\n",
        "\n",
        "            # Translate\n",
        "            translations = evaluator.translate_batch(\n",
        "                texts=source_texts,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            # Evaluate\n",
        "            bleu_score = evaluator.evaluate_translations(\n",
        "                hypotheses=translations,\n",
        "                references=references\n",
        "            )\n",
        "\n",
        "            all_bleu_scores.append(bleu_score.score)\n",
        "            results[file_path] = {\n",
        "                'score': bleu_score.score,\n",
        "                'translations': list(zip(source_texts, translations, references))\n",
        "            }\n",
        "\n",
        "            # Print detailed results\n",
        "            logger.info(f\"\\nDetailed Results for {file_path}:\")\n",
        "            logger.info(\"-\" * 50)\n",
        "            for src, hyp, ref in zip(source_texts, translations, references):\n",
        "                logger.info(f\"\\nSource: {src}\")\n",
        "                logger.info(f\"System: {hyp}\")\n",
        "                logger.info(f\"Reference: {ref[0]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Print summary\n",
        "    logger.info(\"\\nSummary:\")\n",
        "    logger.info(\"-\" * 50)\n",
        "    for file_path, score in zip(jsonl_files, all_bleu_scores):\n",
        "        logger.info(f\"{file_path}: BLEU = {score:.2f}\")\n",
        "\n",
        "    if all_bleu_scores:\n",
        "        avg_score = sum(all_bleu_scores) / len(all_bleu_scores)\n",
        "        logger.info(f\"Average BLEU Score: {avg_score:.2f}\")\n",
        "    else:\n",
        "        logger.warning(\"No scores calculated!\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def read_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Read JSONL file and return parsed data.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to JSONL file\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing parsed data\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        JSONDecodeError: If line contains invalid JSON\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for i, line in enumerate(file, 1):\n",
        "                try:\n",
        "                    data.append(json.loads(line.strip()))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logger.error(f\"Error parsing line {i} in {file_path}: {str(e)}\")\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "SrXJ5-D8EHcc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_files = [\n",
        "    \"01-english-spanish-mapping.jsonl\",\n",
        "    \"02-english-spanish-mapping.jsonl\"\n",
        "]"
      ],
      "metadata": {
        "id": "-dcAMqi-EXO7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API inference example\n",
        "api_config = APIConfig(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "results_api = run_evaluation(\n",
        "    jsonl_files=jsonl_files,\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"Spanish\",\n",
        "    inference_mode=InferenceMode.API,\n",
        "    api_config=api_config,\n",
        "    generation_preset=\"quality\",\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mov5PB2hEbpl",
        "outputId": "9e860b39-2ec5-4a12-b0c3-5128ba8417dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]\n",
            "Translating: 100%|██████████| 24/24 [01:01<00:00,  2.57s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['01-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWaDeMfE-sn",
        "outputId": "4c995d28-7c0a-45fd-acf3-53e8a3518548"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.713797772819994"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['02-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0LaYBe6FUmP",
        "outputId": "15899563-37ed-4767-cfca-23bd82f0c188"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.3643503198117"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('huggingface')"
      ],
      "metadata": {
        "id": "ZQYSBqQFFbKc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HF_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdRPFqjlFjk7",
        "outputId": "4b8de561-199f-4ed4-9d51-23c3d52f4dcb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Local inference\n",
        "results = run_evaluation(\n",
        "    jsonl_files=jsonl_files,\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"Spanish\",\n",
        "    inference_mode=InferenceMode.LOCAL,\n",
        "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    generation_preset=\"quality\",\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4f2c9b5b302d45deb7ead45c7a76d309",
            "c299468f12154abb9bb0f3ab7e6fba28",
            "d91c574d55d74bb5a3a74f60ed4d1668",
            "ca275f2845d141d3b41bebeadededed6",
            "e3f82476b5834434962f4d10446f1faf",
            "61fccdc883ad4dc6a9411b12674998ea",
            "7db77cd078074ee7ba70b1a39a04cdc4",
            "69bc977fd5cc44a2b70951a4687ef385",
            "76bc8aa7a1c24ed6b6a20e9da38c2162",
            "2222519453c243ea8bff451153ceb03b",
            "e0bf386b322c4a02b619f1faa96b5211"
          ]
        },
        "id": "5RaWLvTzElii",
        "outputId": "c17bd6a6-25c7-4cbd-af7b-aa2003cb1f98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f2c9b5b302d45deb7ead45c7a76d309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 8/8 [00:12<00:00,  1.62s/it]\n",
            "Translating: 100%|██████████| 24/24 [00:55<00:00,  2.32s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['01-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-aQSyFPIyam",
        "outputId": "6317f72a-9113-4480-f2a2-d4d777beeedd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.713797772819994"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['02-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gyR0MJqKe5B",
        "outputId": "d5818276-3cad-40eb-e7f4-70570d481c42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.682175159905853"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"File\": [\"01-english-spanish-mapping.jsonl\", \"02-english-spanish-mapping.jsonl\"],\n",
        "    \"Llama 3.1 8B Instruct (API)\": [results_api['01-english-spanish-mapping.jsonl']['score'], results_api['02-english-spanish-mapping.jsonl']['score']],\n",
        "    \"Llama 3.2 3B Instruct (Local)\": [results['01-english-spanish-mapping.jsonl']['score'], results['02-english-spanish-mapping.jsonl']['score']]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "_eLLn2W-V3ae",
        "outputId": "480df3ad-8e0f-4759-9a5e-8d1dd61b885d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                               File  Llama 3.1 8B Instruct (API)  \\\n",
              "0  01-english-spanish-mapping.jsonl                    46.713798   \n",
              "1  02-english-spanish-mapping.jsonl                    21.364350   \n",
              "\n",
              "   Llama 3.2 3B Instruct (Local)  \n",
              "0                      46.713798  \n",
              "1                      10.682175  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b111a8a0-e23b-4836-87bd-b5c2709643b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File</th>\n",
              "      <th>Llama 3.1 8B Instruct (API)</th>\n",
              "      <th>Llama 3.2 3B Instruct (Local)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-english-spanish-mapping.jsonl</td>\n",
              "      <td>46.713798</td>\n",
              "      <td>46.713798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02-english-spanish-mapping.jsonl</td>\n",
              "      <td>21.364350</td>\n",
              "      <td>10.682175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b111a8a0-e23b-4836-87bd-b5c2709643b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b111a8a0-e23b-4836-87bd-b5c2709643b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b111a8a0-e23b-4836-87bd-b5c2709643b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e671b65c-3d54-4f43-a53f-bfefc92628a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e671b65c-3d54-4f43-a53f-bfefc92628a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e671b65c-3d54-4f43-a53f-bfefc92628a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7912cbe0-c128-454d-876f-2ca2fb0baa7e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7912cbe0-c128-454d-876f-2ca2fb0baa7e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"File\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"02-english-spanish-mapping.jsonl\",\n          \"01-english-spanish-mapping.jsonl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Llama 3.1 8B Instruct (API)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.92476619335422,\n        \"min\": 21.3643503198117,\n        \"max\": 46.713797772819994,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          21.3643503198117,\n          46.713797772819994\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Llama 3.2 3B Instruct (Local)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25.47820468674614,\n        \"min\": 10.682175159905853,\n        \"max\": 46.713797772819994,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10.682175159905853,\n          46.713797772819994\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_name = \"Llama 3.2 3B Instruct (Local)\"  # You can change this\n",
        "\n",
        "data = []\n",
        "for file_key, file_data in results.items():\n",
        "    file_prefix = file_key.split('-')[0]  # Extract \"01\" or \"02\"\n",
        "    for translation_data in file_data['translations']:\n",
        "        source = translation_data[0]\n",
        "        translation = translation_data[1]\n",
        "        reference = translation_data[2][0]  # Get the first reference\n",
        "\n",
        "        data.append([model_name, file_prefix, source, translation, reference])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"model\", \"file\", \"source\", \"translation\", \"reference\"])\n",
        "df.to_csv(\"llama32_3B_Instruct_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "guG__vh5fx38"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Llama 3.1 8B Instruct (API)\"  # You can change this\n",
        "\n",
        "data = []\n",
        "for file_key, file_data in results_api.items():\n",
        "    file_prefix = file_key.split('-')[0]  # Extract \"01\" or \"02\"\n",
        "    for translation_data in file_data['translations']:\n",
        "        source = translation_data[0]\n",
        "        translation = translation_data[1]\n",
        "        reference = translation_data[2][0]  # Get the first reference\n",
        "\n",
        "        data.append([model_name, file_prefix, source, translation, reference])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"model\", \"file\", \"source\", \"translation\", \"reference\"])\n",
        "df.to_csv(\"llama31_8B_Instruct_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "JguJfdJmhF48"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}