{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMeQWfUPRX+VzYyDZxcaZ7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c6c9448582e48e89e6ee4e2617263a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e528416f68d54667846546ce8ce69457",
              "IPY_MODEL_a5f04c8f2d23441a8a53489cfd375df3",
              "IPY_MODEL_14ff50ab6cca42e0a88c07bfd786238f"
            ],
            "layout": "IPY_MODEL_ee6104ed02ad45608ad92a3ccaac9e12"
          }
        },
        "e528416f68d54667846546ce8ce69457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa21254101b43c5b46732bfd563691b",
            "placeholder": "​",
            "style": "IPY_MODEL_de221db0b68343b7b6b76e9b2d861fae",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a5f04c8f2d23441a8a53489cfd375df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff1cbaceaaab42a8b6e27793f25c7e39",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_229225f857b44f20b6b756c484a0601f",
            "value": 4
          }
        },
        "14ff50ab6cca42e0a88c07bfd786238f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf7ea8792c5547a092a69d54c1306c43",
            "placeholder": "​",
            "style": "IPY_MODEL_df481ede9f0d4105b076b997f1ddbf17",
            "value": " 4/4 [00:09&lt;00:00,  2.06s/it]"
          }
        },
        "ee6104ed02ad45608ad92a3ccaac9e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa21254101b43c5b46732bfd563691b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de221db0b68343b7b6b76e9b2d861fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff1cbaceaaab42a8b6e27793f25c7e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229225f857b44f20b6b756c484a0601f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf7ea8792c5547a092a69d54c1306c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df481ede9f0d4105b076b997f1ddbf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e866eacd5a34fc3832830b176b999c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afa04a51ac48429da1757bf872e4931b",
              "IPY_MODEL_cdbb7878d2fa459fad4360abc58973e2",
              "IPY_MODEL_a16bf560e1a14a959b0680da3d14daee"
            ],
            "layout": "IPY_MODEL_650bbd2012aa48fe9bc4e5b5f3ece2f4"
          }
        },
        "afa04a51ac48429da1757bf872e4931b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b38355011524e84b30f5e124f2480bd",
            "placeholder": "​",
            "style": "IPY_MODEL_7be13d0305ca4f77bc758e011072459e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cdbb7878d2fa459fad4360abc58973e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_998421dc359f4a00aa9f388b49ffa22c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fc364fa8217472cb124115d75494d88",
            "value": 4
          }
        },
        "a16bf560e1a14a959b0680da3d14daee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98b0013750b74a0cb682986963186b83",
            "placeholder": "​",
            "style": "IPY_MODEL_2d5c0261ebcf4ee8b71e1e057d722b50",
            "value": " 4/4 [00:09&lt;00:00,  2.02s/it]"
          }
        },
        "650bbd2012aa48fe9bc4e5b5f3ece2f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b38355011524e84b30f5e124f2480bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be13d0305ca4f77bc758e011072459e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "998421dc359f4a00aa9f388b49ffa22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc364fa8217472cb124115d75494d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98b0013750b74a0cb682986963186b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5c0261ebcf4ee8b71e1e057d722b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/seamless_sacrebleu_evaluation/blob/main/notebooks/hf_model_sacrebleu_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv8rGGsgBYT4",
        "outputId": "92555333-c38e-4305-dddd-b20c1b3f4f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.10.12 environment at /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 67ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system transformers sacrebleu tqdm torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Any, List, Union\n",
        "from enum import Enum\n",
        "import json\n",
        "import logging\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InferenceMode(Enum):\n",
        "    \"\"\"Enum for inference modes\"\"\"\n",
        "    LOCAL = \"local\"\n",
        "    API = \"api\"\n",
        "\n",
        "@dataclass\n",
        "class APIConfig:\n",
        "    \"\"\"Configuration for API inference\"\"\"\n",
        "    base_url: str\n",
        "    api_key: str\n",
        "    model: str\n",
        "    timeout: int = 30\n",
        "    max_retries: int = 3\n",
        "\n",
        "    def validate(self) -> None:\n",
        "        \"\"\"Validate API configuration\"\"\"\n",
        "        if not self.base_url:\n",
        "            raise ValueError(\"base_url is required for API inference\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"api_key is required for API inference\")\n",
        "        if not self.model:\n",
        "            raise ValueError(\"model is required for API inference\")\n",
        "\n",
        "@dataclass\n",
        "class Llama31Config:\n",
        "    \"\"\"Configuration for Llama 3.1 specific settings\"\"\"\n",
        "    SUPPORTED_LANGUAGES = {\n",
        "        \"English\": \"eng\", \"Spanish\": \"spa\", \"French\": \"fra\",\n",
        "        \"German\": \"deu\", \"Italian\": \"ita\", \"Portuguese\": \"por\",\n",
        "        \"Hindi\": \"hin\", \"Thai\": \"tha\"\n",
        "    }\n",
        "\n",
        "    system_prompt: str = \"\"\"You are a professional translator with expertise in multiple languages.\n",
        "    Your task is to translate the provided text accurately while preserving meaning, tone, and context.\n",
        "    Only provide the direct translation without any explanations or notes.\"\"\"\n",
        "\n",
        "    # Modified template to be more explicit\n",
        "    chat_template: str = (\n",
        "        \"<|begin_of_text|>\"\n",
        "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
        "        \"{system_prompt}\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
        "        \"{user_prompt}\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "    )\n",
        "\n",
        "    def format_prompt(self, text: str, src_lang: str, tgt_lang: str, for_api: bool = False) -> Union[str, List[Dict[str, str]]]:\n",
        "        \"\"\"Format the translation prompt\"\"\"\n",
        "        user_prompt = f\"Translate the following text from {src_lang} to {tgt_lang}. Provide only the translation:\\n\\n{text}\"\n",
        "\n",
        "        if for_api:\n",
        "            return [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        else:\n",
        "            return self.chat_template.format(\n",
        "                system_prompt=self.system_prompt,\n",
        "                user_prompt=user_prompt\n",
        "            )\n",
        "\n",
        "@dataclass\n",
        "class TranslationConfig:\n",
        "    \"\"\"Configuration for translation settings\"\"\"\n",
        "    src_lang: str\n",
        "    tgt_lang: str\n",
        "    inference_mode: InferenceMode = InferenceMode.LOCAL\n",
        "    api_config: Optional[APIConfig] = None\n",
        "\n",
        "    # Generation settings that work for both local and API\n",
        "    max_new_tokens: int = 256\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.95\n",
        "    top_k: int = 50\n",
        "    repetition_penalty: float = 1.2\n",
        "    num_beams: int = 4\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization\"\"\"\n",
        "        if self.inference_mode == InferenceMode.API and not self.api_config:\n",
        "            raise ValueError(\"api_config is required when inference_mode is API\")\n",
        "\n",
        "        if self.api_config:\n",
        "            self.api_config.validate()\n",
        "\n",
        "    def get_api_parameters(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get parameters formatted for API call\"\"\"\n",
        "        return {\n",
        "            \"max_tokens\": self.max_new_tokens,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"presence_penalty\": self.repetition_penalty - 1.0,  # Convert to OpenAI format\n",
        "        }\n",
        "\n",
        "    def get_local_parameters(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get parameters formatted for local inference\"\"\"\n",
        "        return {\n",
        "            \"max_new_tokens\": self.max_new_tokens,\n",
        "            \"num_beams\": self.num_beams,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"repetition_penalty\": self.repetition_penalty,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_preset(\n",
        "        cls,\n",
        "        preset: str,\n",
        "        src_lang: str,\n",
        "        tgt_lang: str,\n",
        "        inference_mode: InferenceMode = InferenceMode.LOCAL,\n",
        "        api_config: Optional[APIConfig] = None\n",
        "    ) -> 'TranslationConfig':\n",
        "        \"\"\"Create config from preset generation strategy\"\"\"\n",
        "        base_config = cls(\n",
        "            src_lang=src_lang,\n",
        "            tgt_lang=tgt_lang,\n",
        "            inference_mode=inference_mode,\n",
        "            api_config=api_config\n",
        "        )\n",
        "\n",
        "        if preset == \"beam_search\":\n",
        "            base_config.num_beams = 5\n",
        "            base_config.temperature = 1.0\n",
        "        elif preset == \"sampling\":\n",
        "            base_config.num_beams = 1\n",
        "            base_config.temperature = 0.7\n",
        "            base_config.top_p = 0.9\n",
        "        elif preset == \"quality\":\n",
        "            base_config.num_beams = 4\n",
        "            base_config.temperature = 0.7\n",
        "            base_config.top_p = 0.95\n",
        "            base_config.repetition_penalty = 1.2\n",
        "\n",
        "        return base_config"
      ],
      "metadata": {
        "id": "E3VMEglxBhWj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APIClient:\n",
        "    \"\"\"Client for making API calls with retry logic\"\"\"\n",
        "\n",
        "    def __init__(self, api_config: APIConfig):\n",
        "        \"\"\"Initialize API client with retry logic\"\"\"\n",
        "        self.api_config = api_config\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self) -> requests.Session:\n",
        "        \"\"\"Create session with retry logic\"\"\"\n",
        "        session = requests.Session()\n",
        "\n",
        "        # Configure retry strategy\n",
        "        retry_strategy = Retry(\n",
        "            total=self.api_config.max_retries,\n",
        "            backoff_factor=0.5,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "        )\n",
        "\n",
        "        # Add retry adapter to session\n",
        "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "\n",
        "        return session\n",
        "\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(3),\n",
        "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "        reraise=True\n",
        "    )\n",
        "    def generate_translation(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        parameters: Dict[str, Any]\n",
        "    ) -> str:\n",
        "        \"\"\"Make API call with retry logic\"\"\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {self.api_config.api_key}\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.api_config.model,\n",
        "            \"messages\": messages,\n",
        "            **parameters\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                f\"{self.api_config.base_url}chat/completions\",\n",
        "                headers=headers,\n",
        "                json=data,\n",
        "                timeout=self.api_config.timeout\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"API call failed: {str(e)}\")\n",
        "            raise RuntimeError(f\"Failed to get translation from API: {str(e)}\")"
      ],
      "metadata": {
        "id": "yGx67JtZBqto"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sacrebleu import BLEU\n",
        "import tqdm\n",
        "\n",
        "class Llama31TranslationEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: TranslationConfig,\n",
        "        model_name: Optional[str] = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        debug: bool = False\n",
        "    ):\n",
        "        \"\"\"Initialize the translation evaluator\"\"\"\n",
        "        self.debug = debug\n",
        "        self.config = config\n",
        "        self.llama_config = Llama31Config()\n",
        "\n",
        "        if config.inference_mode == InferenceMode.API:\n",
        "            logger.info(\"Initializing API client...\")\n",
        "            self.api_client = APIClient(config.api_config)\n",
        "            self.inference_fn = self._translate_api\n",
        "        else:\n",
        "            logger.info(f\"Loading local model on device: {device}\")\n",
        "            self.device = device\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            # Handle tokenizer settings\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.padding_side = \"left\"\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            self.inference_fn = self._translate_local\n",
        "\n",
        "        self.bleu = BLEU()\n",
        "        logger.info(\"Setup complete!\")\n",
        "\n",
        "    def _translate_api(self, text: str, batch_idx: int = 0) -> str:\n",
        "        \"\"\"Translate using API endpoint\"\"\"\n",
        "        try:\n",
        "            messages = self.llama_config.format_prompt(\n",
        "                text=text,\n",
        "                src_lang=self.config.src_lang,\n",
        "                tgt_lang=self.config.tgt_lang,\n",
        "                for_api=True  # Get API format\n",
        "            )\n",
        "\n",
        "            parameters = self.config.get_api_parameters()\n",
        "            translation = self.api_client.generate_translation(messages, parameters)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"API translation for batch {batch_idx}: {translation}\")\n",
        "\n",
        "            return translation.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"API translation failed for batch {batch_idx}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _translate_local(self, text: str, batch_idx: int = 0) -> str:\n",
        "        \"\"\"Translate using local model with detailed debugging\"\"\"\n",
        "        try:\n",
        "            # Generate the prompt\n",
        "            prompt = self.llama_config.format_prompt(\n",
        "                text=text,\n",
        "                src_lang=self.config.src_lang,\n",
        "                tgt_lang=self.config.tgt_lang,\n",
        "                for_api=False\n",
        "            )\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"\\nProcessing batch {batch_idx}\")\n",
        "                logger.info(f\"Input text: {text}\")\n",
        "                logger.info(f\"Generated prompt:\\n{prompt}\")\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            ).to(self.device)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Input token count: {len(inputs['input_ids'][0])}\")\n",
        "\n",
        "            # Setup generation parameters\n",
        "            parameters = self.config.get_local_parameters()\n",
        "            parameters.update({\n",
        "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "                \"do_sample\": parameters[\"temperature\"] > 0,\n",
        "            })\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Generation parameters: {parameters}\")\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(**inputs, **parameters)\n",
        "\n",
        "            # Decode full output\n",
        "            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Full model output:\\n{full_output}\")\n",
        "\n",
        "            # Extract translation - look for assistant's response\n",
        "            assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "            eot_marker = \"<|eot_id|>\"\n",
        "\n",
        "            if assistant_marker in full_output:\n",
        "                response_start = full_output.find(assistant_marker) + len(assistant_marker)\n",
        "                response_end = full_output.find(eot_marker, response_start)\n",
        "\n",
        "                if response_end != -1:\n",
        "                    translation = full_output[response_start:response_end]\n",
        "                else:\n",
        "                    translation = full_output[response_start:]\n",
        "            else:\n",
        "                # Fallback: try to extract everything after the prompt\n",
        "                translation = full_output[len(prompt):]\n",
        "\n",
        "            # Clean up the translation\n",
        "            translation = translation.replace(assistant_marker, \"\").replace(eot_marker, \"\")\n",
        "            translation = translation.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "\n",
        "            if self.debug:\n",
        "                logger.info(f\"Extracted translation: {translation}\")\n",
        "                if not translation:\n",
        "                    logger.warning(\"Warning: Empty translation detected!\")\n",
        "\n",
        "            return translation\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Local translation failed for batch {batch_idx}: {str(e)}\")\n",
        "            logger.error(f\"Error details:\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def translate_batch(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        batch_size: int = 1\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Translate a list of texts\"\"\"\n",
        "        translations = []\n",
        "\n",
        "        for i in tqdm.trange(0, len(texts), batch_size, desc=\"Translating\"):\n",
        "            batch = texts[i:i + batch_size]\n",
        "\n",
        "            batch_translations = []\n",
        "            for j, text in enumerate(batch):\n",
        "                try:\n",
        "                    translation = self.inference_fn(text, i + j)\n",
        "                    batch_translations.append(translation)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Translation failed for text {i + j}: {str(e)}\")\n",
        "                    batch_translations.append(\"\")\n",
        "\n",
        "            translations.extend(batch_translations)\n",
        "\n",
        "        return translations\n",
        "\n",
        "    def evaluate_translations(\n",
        "        self,\n",
        "        hypotheses: List[str],\n",
        "        references: Union[List[str], List[List[str]]],\n",
        "        verbose: bool = True\n",
        "    ) -> BLEU:\n",
        "        \"\"\"Evaluate translations using sacreBLEU\"\"\"\n",
        "        if isinstance(references[0], str):\n",
        "            references = [[ref] for ref in references]\n",
        "\n",
        "        bleu_score = self.bleu.corpus_score(hypotheses, references)\n",
        "\n",
        "        if verbose:\n",
        "            logger.info(f\"BLEU score: {bleu_score.score:.2f}\")\n",
        "            logger.info(f\"Signature: {self.bleu.get_signature()}\")\n",
        "\n",
        "        return bleu_score"
      ],
      "metadata": {
        "id": "ItGE5uxYBsxB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "BASE_URL = userdata.get('MODAL_BASE_URL') # should end in /v1/\n",
        "API_KEY = userdata.get('DSBA_LLAMA3_KEY')\n",
        "model_name = \"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "5SCReLkACCAv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        "### API"
      ],
      "metadata": {
        "id": "spZ3OkU9Jobt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test data\n",
        "source_texts = [\n",
        "    \"Hello, my dog is cute\",\n",
        "    \"The weather is nice today\",\n",
        "    \"I love programming\"\n",
        "]\n",
        "\n",
        "# Reference translations in French\n",
        "references = [\n",
        "    [\"Bonjour, mon chien est mignon\"],\n",
        "    [\"Le temps est beau aujourd'hui\"],\n",
        "    [\"J'aime la programmation\"]\n",
        "]\n",
        "\n",
        "# API Configuration\n",
        "api_config = APIConfig(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "# Translation Configuration\n",
        "config = TranslationConfig.from_preset(\n",
        "    preset=\"sampling\",\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"French\",\n",
        "    inference_mode=InferenceMode.API,\n",
        "    api_config=api_config\n",
        ")\n",
        "\n",
        "print(\"Initializing translator...\")\n",
        "# Initialize evaluator with API mode\n",
        "evaluator = Llama31TranslationEvaluator(\n",
        "    config=config,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\nTranslating texts...\")\n",
        "# Translate\n",
        "translations = evaluator.translate_batch(\n",
        "    texts=source_texts,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for src, trans, ref in zip(source_texts, translations, references):\n",
        "    print(f\"\\nSource: {src}\")\n",
        "    print(f\"\\nTranslation: {trans}\")\n",
        "    print(f\"Reference: {ref[0]}\")\n",
        "\n",
        "# Evaluate translations\n",
        "bleu_score = evaluator.evaluate_translations(\n",
        "    hypotheses=translations,\n",
        "    references=references,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg0LDXF7B-WB",
        "outputId": "b359c250-d432-4add-b5ec-04822c23b615"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing translator...\n",
            "\n",
            "Translating texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "\n",
            "Source: Hello, my dog is cute\n",
            "\n",
            "Translation: Bonjour, mon chien est mignon.\n",
            "Reference: Bonjour, mon chien est mignon\n",
            "\n",
            "Source: The weather is nice today\n",
            "\n",
            "Translation: Le temps est agréable aujourd'hui.\n",
            "Reference: Le temps est beau aujourd'hui\n",
            "\n",
            "Source: I love programming\n",
            "\n",
            "Translation: J'adore le programmation\n",
            "Reference: J'aime la programmation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pPcvRRYDWqI",
        "outputId": "0d789b03-d974-41c7-d2b6-dfebe49283da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 80.91 85.7/83.3/80.0/75.0 (BP = 1.000 ratio = 1.167 hyp_len = 7 ref_len = 6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local"
      ],
      "metadata": {
        "id": "iv1_Hd4bJrZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a single translation first\n",
        "test_text = \"Hello, my dog is cute\"\n",
        "\n",
        "# Initialize with debug mode\n",
        "config = TranslationConfig.from_preset(\n",
        "    preset=\"quality\",\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"French\",\n",
        "    inference_mode=InferenceMode.LOCAL\n",
        ")\n",
        "\n",
        "print(\"Initializing translator...\")\n",
        "# Initialize evaluator with API mode\n",
        "evaluator = Llama31TranslationEvaluator(\n",
        "    config=config,\n",
        "    model_name=\"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\nTranslating texts...\")\n",
        "# Translate\n",
        "translations = evaluator.translate_batch(\n",
        "    texts=source_texts,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for src, trans, ref in zip(source_texts, translations, references):\n",
        "    print(f\"\\nSource: {src}\")\n",
        "    print(f\"\\nTranslation: {trans}\")\n",
        "    print(f\"Reference: {ref[0]}\")\n",
        "\n",
        "# Evaluate translations\n",
        "bleu_score = evaluator.evaluate_translations(\n",
        "    hypotheses=translations,\n",
        "    references=references,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "5c6c9448582e48e89e6ee4e2617263a8",
            "e528416f68d54667846546ce8ce69457",
            "a5f04c8f2d23441a8a53489cfd375df3",
            "14ff50ab6cca42e0a88c07bfd786238f",
            "ee6104ed02ad45608ad92a3ccaac9e12",
            "3aa21254101b43c5b46732bfd563691b",
            "de221db0b68343b7b6b76e9b2d861fae",
            "ff1cbaceaaab42a8b6e27793f25c7e39",
            "229225f857b44f20b6b756c484a0601f",
            "cf7ea8792c5547a092a69d54c1306c43",
            "df481ede9f0d4105b076b997f1ddbf17"
          ]
        },
        "id": "01FIV4JkJsmr",
        "outputId": "98117d9f-05bb-4999-85ef-52366ee157ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing translator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c6c9448582e48e89e6ee4e2617263a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translating texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "\n",
            "Source: Hello, my dog is cute\n",
            "\n",
            "Translation: Bonjour, mon chien est mignon.\n",
            "Reference: Bonjour, mon chien est mignon\n",
            "\n",
            "Source: The weather is nice today\n",
            "\n",
            "Translation: Le temps est agréable aujourd'hui.\n",
            "Reference: Le temps est beau aujourd'hui\n",
            "\n",
            "Source: I love programming\n",
            "\n",
            "Translation: J'adore programmer\n",
            "Reference: J'aime la programmation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgl8GvmyK-To",
        "outputId": "89af5a5e-77ca-4665-8b4b-e7f6c6d3e111"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 80.91 85.7/83.3/80.0/75.0 (BP = 1.000 ratio = 1.167 hyp_len = 7 ref_len = 6)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "laySEAoHEIN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/01-english-spanish-mapping.jsonl\n",
        "!wget https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/02-english-spanish-mapping.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kShoIGj4ELDO",
        "outputId": "8b8edea6-2d69-4092-8fd7-2374df078c00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 16:58:46--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/01-english-spanish-mapping.jsonl\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/01-english-spanish-mapping.jsonl [following]\n",
            "--2024-12-01 16:58:46--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/01-english-spanish-mapping.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2829 (2.8K) [text/plain]\n",
            "Saving to: ‘01-english-spanish-mapping.jsonl.2’\n",
            "\n",
            "01-english-spanish- 100%[===================>]   2.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 16:58:46 (53.2 MB/s) - ‘01-english-spanish-mapping.jsonl.2’ saved [2829/2829]\n",
            "\n",
            "--2024-12-01 16:58:46--  https://github.com/wesslen/seamless_sacrebleu_evaluation/raw/main/data/02-english-spanish-mapping.jsonl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/02-english-spanish-mapping.jsonl [following]\n",
            "--2024-12-01 16:58:46--  https://raw.githubusercontent.com/wesslen/seamless_sacrebleu_evaluation/main/data/02-english-spanish-mapping.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12284 (12K) [text/plain]\n",
            "Saving to: ‘02-english-spanish-mapping.jsonl.2’\n",
            "\n",
            "02-english-spanish- 100%[===================>]  12.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-01 16:58:46 (119 MB/s) - ‘02-english-spanish-mapping.jsonl.2’ saved [12284/12284]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def run_evaluation(\n",
        "    jsonl_files: List[str],\n",
        "    src_lang: str = \"English\",\n",
        "    tgt_lang: str = \"Spanish\",\n",
        "    inference_mode: InferenceMode = InferenceMode.LOCAL,\n",
        "    model_name: Optional[str] = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    api_config: Optional[APIConfig] = None,\n",
        "    batch_size: int = 4,\n",
        "    generation_preset: str = \"quality\",\n",
        "    debug: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Run evaluation pipeline with support for both local and API inference.\n",
        "\n",
        "    Args:\n",
        "        jsonl_files: List of JSONL files containing test data\n",
        "        src_lang: Source language\n",
        "        tgt_lang: Target language\n",
        "        inference_mode: Whether to use local model or API endpoint\n",
        "        model_name: HuggingFace model identifier (for local inference)\n",
        "        api_config: API configuration (for API inference)\n",
        "        batch_size: Batch size for processing\n",
        "        generation_preset: Generation strategy preset\n",
        "        debug: Enable debug logging\n",
        "    \"\"\"\n",
        "    logger.info(f\"\\nInitializing translator in {inference_mode.value} mode...\")\n",
        "\n",
        "    # Create translation config\n",
        "    config = TranslationConfig.from_preset(\n",
        "        preset=generation_preset,\n",
        "        src_lang=src_lang,\n",
        "        tgt_lang=tgt_lang,\n",
        "        inference_mode=inference_mode,\n",
        "        api_config=api_config\n",
        "    )\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = Llama31TranslationEvaluator(\n",
        "        config=config,\n",
        "        model_name=model_name if inference_mode == InferenceMode.LOCAL else None,\n",
        "        debug=debug\n",
        "    )\n",
        "\n",
        "    all_bleu_scores = []\n",
        "    results = {}\n",
        "\n",
        "    for file_path in jsonl_files:\n",
        "        logger.info(f\"\\nProcessing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Load data\n",
        "            mapping = read_jsonl(file_path)\n",
        "            source_texts = [item[\"source_text\"] for item in mapping]\n",
        "            references = [item[\"references\"] for item in mapping]\n",
        "\n",
        "            # Translate\n",
        "            translations = evaluator.translate_batch(\n",
        "                texts=source_texts,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            # Evaluate\n",
        "            bleu_score = evaluator.evaluate_translations(\n",
        "                hypotheses=translations,\n",
        "                references=references\n",
        "            )\n",
        "\n",
        "            all_bleu_scores.append(bleu_score.score)\n",
        "            results[file_path] = {\n",
        "                'score': bleu_score.score,\n",
        "                'translations': list(zip(source_texts, translations, references))\n",
        "            }\n",
        "\n",
        "            # Print detailed results\n",
        "            logger.info(f\"\\nDetailed Results for {file_path}:\")\n",
        "            logger.info(\"-\" * 50)\n",
        "            for src, hyp, ref in zip(source_texts, translations, references):\n",
        "                logger.info(f\"\\nSource: {src}\")\n",
        "                logger.info(f\"System: {hyp}\")\n",
        "                logger.info(f\"Reference: {ref[0]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Print summary\n",
        "    logger.info(\"\\nSummary:\")\n",
        "    logger.info(\"-\" * 50)\n",
        "    for file_path, score in zip(jsonl_files, all_bleu_scores):\n",
        "        logger.info(f\"{file_path}: BLEU = {score:.2f}\")\n",
        "\n",
        "    if all_bleu_scores:\n",
        "        avg_score = sum(all_bleu_scores) / len(all_bleu_scores)\n",
        "        logger.info(f\"Average BLEU Score: {avg_score:.2f}\")\n",
        "    else:\n",
        "        logger.warning(\"No scores calculated!\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def read_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Read JSONL file and return parsed data.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to JSONL file\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing parsed data\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        JSONDecodeError: If line contains invalid JSON\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for i, line in enumerate(file, 1):\n",
        "                try:\n",
        "                    data.append(json.loads(line.strip()))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logger.error(f\"Error parsing line {i} in {file_path}: {str(e)}\")\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found: {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "SrXJ5-D8EHcc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_files = [\n",
        "    \"01-english-spanish-mapping.jsonl\",\n",
        "    \"02-english-spanish-mapping.jsonl\"\n",
        "]"
      ],
      "metadata": {
        "id": "-dcAMqi-EXO7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API inference example\n",
        "api_config = APIConfig(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "results_api = run_evaluation(\n",
        "    jsonl_files=jsonl_files,\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"Spanish\",\n",
        "    inference_mode=InferenceMode.API,\n",
        "    api_config=api_config,\n",
        "    generation_preset=\"quality\",\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mov5PB2hEbpl",
        "outputId": "f65f7e5d-9c73-4cac-83a1-e4ecb1507079"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\n",
            "Translating: 100%|██████████| 24/24 [00:58<00:00,  2.42s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['01-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWaDeMfE-sn",
        "outputId": "51b2c365-bb1c-4ec4-f04d-0379da7fb043"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.713797772819994"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['02-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0LaYBe6FUmP",
        "outputId": "40c99b77-1308-4c81-ca55-93f392581fdd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.058533129758727"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('huggingface')"
      ],
      "metadata": {
        "id": "ZQYSBqQFFbKc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HF_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdRPFqjlFjk7",
        "outputId": "f51fff74-5040-4b9b-eb1d-ea192807198e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Local inference\n",
        "results = run_evaluation(\n",
        "    jsonl_files=jsonl_files,\n",
        "    src_lang=\"English\",\n",
        "    tgt_lang=\"Spanish\",\n",
        "    inference_mode=InferenceMode.LOCAL,\n",
        "    model_name=\"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    generation_preset=\"quality\",\n",
        "    debug=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "3e866eacd5a34fc3832830b176b999c0",
            "afa04a51ac48429da1757bf872e4931b",
            "cdbb7878d2fa459fad4360abc58973e2",
            "a16bf560e1a14a959b0680da3d14daee",
            "650bbd2012aa48fe9bc4e5b5f3ece2f4",
            "6b38355011524e84b30f5e124f2480bd",
            "7be13d0305ca4f77bc758e011072459e",
            "998421dc359f4a00aa9f388b49ffa22c",
            "0fc364fa8217472cb124115d75494d88",
            "98b0013750b74a0cb682986963186b83",
            "2d5c0261ebcf4ee8b71e1e057d722b50"
          ]
        },
        "id": "5RaWLvTzElii",
        "outputId": "ea30aaee-75b9-473a-e037-85d3e436ce72"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e866eacd5a34fc3832830b176b999c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 8/8 [00:14<00:00,  1.79s/it]\n",
            "Translating: 100%|██████████| 24/24 [01:05<00:00,  2.72s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['01-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-aQSyFPIyam",
        "outputId": "f7afbae8-c55b-4d44-c9a8-0eea8c0c0023"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.713797772819994"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_api['02-english-spanish-mapping.jsonl']['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gyR0MJqKe5B",
        "outputId": "dd98d943-9532-48b4-c5ec-a6d4e0643921"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.058533129758727"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}